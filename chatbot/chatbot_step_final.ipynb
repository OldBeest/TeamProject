{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가상환경설정\n",
    "    아나콘다 가상환경 설정하기\n",
    "  - conda create -n med_chatbot python=3.9\n",
    "  - conda activate  med_chatbot\n",
    "  - pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 (cpu만 사용할때)\n",
    "  - pip install python-mecab-ko\n",
    "  - pip install sentence-transformers\n",
    "  - pip install pandas\n",
    "  - pip install matplotlib\n",
    "  - pip install numpy==1.26.4 (제일 중요!(numpy가 2.0.1로 깔려있지만, 원활한 함수 사용을 위해 1.26.4로 다운그레이드 해줘야함))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KOREAVC\\anaconda3\\envs\\med_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import util\n",
    "from mecab import MeCab\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json 데이터 불러와 데이터 프레임 만들기\n",
    " 1. glob으로 해당 폴더 모든 json 파일경로를 list로 불러옴\n",
    " 2. json.load를 이용해 질문, 답변, 의도를 각각 리스트에 모두 담는다\n",
    " 3. 질문, 답변, 의도에 대한 데이터 프레임을 만들고 concat으로 하나의 데이터프레임으로 합친다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(path1, path2):\n",
    "    question_path = path1\n",
    "    answer_path = path2\n",
    "\n",
    "    return glob(question_path + '/*'), glob(answer_path + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_q_path, disease_a_path = all_data('./training/라벨링데이터/질문/' , './training/라벨링데이터/답변/') # 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./training/라벨링데이터/질문\\\\알츠하이머병',\n",
       " './training/라벨링데이터/질문\\\\알코올성 치매',\n",
       " './training/라벨링데이터/질문\\\\우울증',\n",
       " './training/라벨링데이터/질문\\\\치매']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_q_path # q_data : 의도별 폴더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list = []\n",
    "a_list = []\n",
    "i_list = []\n",
    "for intention_q_path, intention_a_path in zip(disease_q_path, disease_a_path):\n",
    "    intention_q_folders, intention_a_folders = glob(intention_q_path + '/*'), glob(intention_a_path + '/*')\n",
    "    \n",
    "    for q_intention, a_intention  in zip(intention_q_folders, intention_a_folders):\n",
    "        q_json_files = glob(q_intention + '/*')\n",
    "        a_json_files = glob(a_intention + '/*')\n",
    "        \n",
    "        # print(q_intention, len(q_json_files), a_intention, len(a_json_files))\n",
    "        # print(min([len(q_json_files), len(a_json_files)]))\n",
    "        \n",
    "        for idx in range(min([len(q_json_files), len(a_json_files)])):\n",
    "            with open(q_json_files[idx], 'r', encoding='utf-8') as file:\n",
    "                q_json_data = json.load(file)\n",
    "                q_list.append(q_json_data['question'])\n",
    "                i_list.append(q_json_data['intention'])\n",
    "            with open(a_json_files[idx], 'r', encoding='utf-8') as file:\n",
    "                a_json_data = json.load(file)\n",
    "                sentence = \"\"\n",
    "                for key in a_json_data['answer'].keys():\n",
    "                    sentence += a_json_data['answer'][key]\n",
    "                a_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.DataFrame(q_list)\n",
    "i_df = pd.DataFrame(i_list)\n",
    "a_df = pd.DataFrame(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((q_df, i_df, a_df), axis=1)\n",
    "df.columns = ['question', 'intention', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dementia_fix.csv', index_col=0)\n",
    "df.columns = ['question', 'intention', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>intention</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>알츠하이머병의 원인으로 매일 소주를 섭취하는 것이 언급되고 있는데, 이에 대한 근거...</td>\n",
       "      <td>원인</td>\n",
       "      <td>알츠하이머병의 정확한 원인은 아직 밝혀지지 않았지만, 연구들이 알츠하이머병의 발병 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>알츠하이머병이라는 질병은 유전적 영향을 받는 것인가요?</td>\n",
       "      <td>원인</td>\n",
       "      <td>알츠하이머병은 현재까지 완전한 원인이 밝혀지지 않았습니다.알츠하이머병은 아직 완전히...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>알츠하이머병의 발생 원인에 대한 연구나 발견이 진행 중인가요?</td>\n",
       "      <td>원인</td>\n",
       "      <td>알츠하이머병은 치매를 일으키는 가장 흔한 퇴행성 뇌질환으로, 1907년 독일 의사 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>알츠하이머병의 발병과 관련하여 뇌의 노화로 인한 증상과 원인을 알려주세요.</td>\n",
       "      <td>원인</td>\n",
       "      <td>알츠하이머병은 현재까지 그 발병 원인에 대한 완벽한 해명은 아직 이루어지지 않았습니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>알츠하이머병의 원인과 관련된 연구 결과가 있을까요? 알려주세요.</td>\n",
       "      <td>원인</td>\n",
       "      <td>알츠하이머병은 복잡한 질환으로, 아직도 원인이 완전히 밝혀진 것은 아닙니다. 그러나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>치매 치료에는 어떤 운동이나 작업이 효과적일까요?</td>\n",
       "      <td>치료</td>\n",
       "      <td>치매는 노인들에게 주로 발생하는 뇌질환으로, 원인과 치료 방법은 아직 완전히 밝혀진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6619</th>\n",
       "      <td>치매 치료의 결과와 과정을 상세히 설명해주세요. 치매 치료의 효과는 어떻게 나타날까요?</td>\n",
       "      <td>치료</td>\n",
       "      <td>치매는 일상 생활을 수행하는 능력을 심각하게 손상시키는 질환으로, 후천성 치매와 노...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6620</th>\n",
       "      <td>치매를 치료하기 위해 어떤 치료 방법들이 효과적일까요?</td>\n",
       "      <td>치료</td>\n",
       "      <td>치매는 노화로 인해 기억력과 지능을 점차적으로 잃는 질병으로, 알츠하이머병이 주요한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6621</th>\n",
       "      <td>치매 치료를 위해 어떤 약물이 사용될 수 있을까요?</td>\n",
       "      <td>치료</td>\n",
       "      <td>알츠하이머병은 뇌에 변화가 생겨서 인지 기능에 장애가 생기는 신경퇴행성 질환입니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622</th>\n",
       "      <td>치매 치료를 위해 어떤 전문가와 협력해야 할까요?</td>\n",
       "      <td>치료</td>\n",
       "      <td>치매는 현재까지 완전한 치료가 불가능한 치매입니다. 치매는 다양한 원인에 의해 발생...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6623 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question intention  \\\n",
       "0     알츠하이머병의 원인으로 매일 소주를 섭취하는 것이 언급되고 있는데, 이에 대한 근거...        원인   \n",
       "1                        알츠하이머병이라는 질병은 유전적 영향을 받는 것인가요?        원인   \n",
       "2                    알츠하이머병의 발생 원인에 대한 연구나 발견이 진행 중인가요?        원인   \n",
       "3             알츠하이머병의 발병과 관련하여 뇌의 노화로 인한 증상과 원인을 알려주세요.        원인   \n",
       "4                   알츠하이머병의 원인과 관련된 연구 결과가 있을까요? 알려주세요.        원인   \n",
       "...                                                 ...       ...   \n",
       "6618                        치매 치료에는 어떤 운동이나 작업이 효과적일까요?        치료   \n",
       "6619   치매 치료의 결과와 과정을 상세히 설명해주세요. 치매 치료의 효과는 어떻게 나타날까요?        치료   \n",
       "6620                     치매를 치료하기 위해 어떤 치료 방법들이 효과적일까요?        치료   \n",
       "6621                       치매 치료를 위해 어떤 약물이 사용될 수 있을까요?        치료   \n",
       "6622                        치매 치료를 위해 어떤 전문가와 협력해야 할까요?        치료   \n",
       "\n",
       "                                                 answer  \n",
       "0     알츠하이머병의 정확한 원인은 아직 밝혀지지 않았지만, 연구들이 알츠하이머병의 발병 ...  \n",
       "1     알츠하이머병은 현재까지 완전한 원인이 밝혀지지 않았습니다.알츠하이머병은 아직 완전히...  \n",
       "2     알츠하이머병은 치매를 일으키는 가장 흔한 퇴행성 뇌질환으로, 1907년 독일 의사 ...  \n",
       "3     알츠하이머병은 현재까지 그 발병 원인에 대한 완벽한 해명은 아직 이루어지지 않았습니...  \n",
       "4     알츠하이머병은 복잡한 질환으로, 아직도 원인이 완전히 밝혀진 것은 아닙니다. 그러나...  \n",
       "...                                                 ...  \n",
       "6618  치매는 노인들에게 주로 발생하는 뇌질환으로, 원인과 치료 방법은 아직 완전히 밝혀진...  \n",
       "6619  치매는 일상 생활을 수행하는 능력을 심각하게 손상시키는 질환으로, 후천성 치매와 노...  \n",
       "6620  치매는 노화로 인해 기억력과 지능을 점차적으로 잃는 질병으로, 알츠하이머병이 주요한...  \n",
       "6621  알츠하이머병은 뇌에 변화가 생겨서 인지 기능에 장애가 생기는 신경퇴행성 질환입니다....  \n",
       "6622  치매는 현재까지 완전한 치료가 불가능한 치매입니다. 치매는 다양한 원인에 의해 발생...  \n",
       "\n",
       "[6623 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"dementia_fix.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# df.to_csv(\"dementia_shuffle.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_data(path1, path2):\n",
    "#     question_path = path1\n",
    "#     answer_path = path2\n",
    "\n",
    "#     return glob(question_path + '/*/*.json'), glob(answer_path + '/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_data, a_data = all_data('./dementia/training/원천데이터/질문/치매', './dementia/training/원천데이터/답변/치매') # 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 치매 데이터 개수 확인 (질문, 답변)\n",
    "# len(q_data), len(a_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(q_data[1000], 'r', encoding='utf-8') as file:\n",
    "#     json_data = json.load(file)\n",
    "#     print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_list = []\n",
    "# for i in range(len(q_data)):\n",
    "#     with open(q_data[i], 'r', encoding='utf-8') as file:\n",
    "#         json_data = json.load(file)\n",
    "#         q_list.append(json_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_list = []\n",
    "# for i in range(len(q_data)):\n",
    "#     with open(a_data[i], 'r', encoding='utf-8') as file:\n",
    "#         json_data = json.load(file)\n",
    "#         sentence = \"\"\n",
    "#         for key in json_data['answer']:\n",
    "#             sentence += json_data['answer'][key]\n",
    "#         a_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_list=[]\n",
    "# for i in range(len(q_data)):\n",
    "#     with open(q_data[i],'r',encoding='utf-8') as file:\n",
    "#         json_data=json.load(file)\n",
    "#         sentence = \"\"\n",
    "#         sentence += json_data['intention']\n",
    "#         i_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dementia_shuffle.csv', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 프레임 만들기\n",
    "# q_df = pd.DataFrame(q_list) # 질문 데이터 프레임\n",
    "# a_df = pd.DataFrame(a_list) # 답변 데이터 프레임\n",
    "# i_df = pd.DataFrame(i_list) # 의도 데이터 프레임\n",
    "# qa_df = pd.concat((q_df, a_df), axis=1) # 질문-의도 데이터 프레임\n",
    "# qa_df.columns=['question', 'answer']\n",
    "# qi_df = pd.concat((q_df, i_df), axis=1) # 질문-답변 데이터 프레임\n",
    "# qi_df.columns=['question', 'intention']\n",
    "# df = pd.concat((q_df, i_df, a_df), axis=1) # 질문-의도-답변 데이터 프레임\n",
    "# df.columns=['question', 'intention', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv 파일로 저장\n",
    "# df.to_csv(\"dementia_qia.csv\", sep=',') #질문, 의도, 답변 포함\n",
    "# qa_df.to_csv(\"dementia_qa.csv\", sep=',') #질문, 답변\n",
    "# qi_df.to_csv(\"dementia_qi.csv\", sep=',') #질문, 의도 -> 나중에 의도 분류 모델에 활용할수도 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data = df['question']\n",
    "a_data = df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       알츠하이머병의 원인으로 매일 소주를 섭취하는 것이 언급되고 있는데, 이에 대한 근거...\n",
       "1                          알츠하이머병이라는 질병은 유전적 영향을 받는 것인가요?\n",
       "2                      알츠하이머병의 발생 원인에 대한 연구나 발견이 진행 중인가요?\n",
       "3               알츠하이머병의 발병과 관련하여 뇌의 노화로 인한 증상과 원인을 알려주세요.\n",
       "4                     알츠하이머병의 원인과 관련된 연구 결과가 있을까요? 알려주세요.\n",
       "                              ...                        \n",
       "6618                          치매 치료에는 어떤 운동이나 작업이 효과적일까요?\n",
       "6619     치매 치료의 결과와 과정을 상세히 설명해주세요. 치매 치료의 효과는 어떻게 나타날까요?\n",
       "6620                       치매를 치료하기 위해 어떤 치료 방법들이 효과적일까요?\n",
       "6621                         치매 치료를 위해 어떤 약물이 사용될 수 있을까요?\n",
       "6622                          치매 치료를 위해 어떤 전문가와 협력해야 할까요?\n",
       "Name: question, Length: 6623, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6623"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석 불러오기\n",
    "from mecab import MeCab\n",
    "m = MeCab()\n",
    "\n",
    "# 각 질문마다 형태소 분석을 통해 질문당 몇 개의 단어 토큰이 들어갔는지 count리스트에 담음\n",
    "count = []\n",
    "for q in q_data:\n",
    "        count_num = len(m.morphs(q))\n",
    "        count.append(count_num)\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       알츠하이머병의 정확한 원인은 아직 밝혀지지 않았지만, 연구들이 알츠하이머병의 발병 ...\n",
       "1       알츠하이머병은 현재까지 완전한 원인이 밝혀지지 않았습니다.알츠하이머병은 아직 완전히...\n",
       "2       알츠하이머병은 치매를 일으키는 가장 흔한 퇴행성 뇌질환으로, 1907년 독일 의사 ...\n",
       "3       알츠하이머병은 현재까지 그 발병 원인에 대한 완벽한 해명은 아직 이루어지지 않았습니...\n",
       "4       알츠하이머병은 복잡한 질환으로, 아직도 원인이 완전히 밝혀진 것은 아닙니다. 그러나...\n",
       "                              ...                        \n",
       "6618    치매는 노인들에게 주로 발생하는 뇌질환으로, 원인과 치료 방법은 아직 완전히 밝혀진...\n",
       "6619    치매는 일상 생활을 수행하는 능력을 심각하게 손상시키는 질환으로, 후천성 치매와 노...\n",
       "6620    치매는 노화로 인해 기억력과 지능을 점차적으로 잃는 질병으로, 알츠하이머병이 주요한...\n",
       "6621    알츠하이머병은 뇌에 변화가 생겨서 인지 기능에 장애가 생기는 신경퇴행성 질환입니다....\n",
       "6622    치매는 현재까지 완전한 치료가 불가능한 치매입니다. 치매는 다양한 원인에 의해 발생...\n",
       "Name: answer, Length: 6623, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df['question']\n",
    "answer = df['answer']\n",
    "intention = df['intention']\n",
    "df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6623"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 질문마다 형태소 분석을 통해 답변당 몇 개의 단어 토큰이 들어갔는지 count1 리스트에 담음\n",
    "\n",
    "count1 = []\n",
    "for a in a_data:\n",
    "    count_num = len(m.morphs(a))\n",
    "    count1.append(count_num)\n",
    "len(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6. 16. 19. 22. 25. 30.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 질문에 사용된 단어 개수\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(count)\n",
    "point_6 = np.percentile(count, q=[0, 50, 75, 90, 95, 99]) # 상위 0%, 50%, 75%, 90%, 95%, 99% 구간으로 나눠서 분포 그리기\n",
    "print(point_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 66. 185. 216. 251. 277. 350.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 답변에 사용된 단어 개수 ()\n",
    "\n",
    "plt.hist(count1)\n",
    "point1_6 = np.percentile(count1, q=[0, 50, 75, 90, 95, 99]) # 상위 0%, 50%, 75%, 90%, 95%, 99% 구간으로 나눠서 분포 그리기\n",
    "print(point1_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[^ ?,.!A-Za-z0-9가-힣+]', re.UNICODE)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 한글, 영어, 숫자, 공백, ?!.,을 제외한 나머지 문자 제거\n",
    "korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "\n",
    "# 패턴 컴파일\n",
    "normalizer = re.compile(korean_pattern)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수정 전: 알츠하이머병의 원인이 30대에서 더 쉽게 찾아볼 수 있는 이유가 있나요?\n",
      "수정 후: 알츠하이머병의 원인이 30대에서 더 쉽게 찾아볼 수 있는 이유가 있나요?\n",
      "수정 전: 알츠하이머병은 노화된 단백질이 뇌세포를 파괴하는 퇴행성 신경 질환입니다. 이 질환은 노화에 따른 뇌신경세포의 죽음으로 인해 발생하며, 정확한 원인은 알려져 있지 않습니다. 초기 증상은 경미한 기억 장애부터 언어 장애, 행동 변화 등으로 나타납니다.알츠하이머병은 점진적인 기억력 감퇴와 언어 장애, 실행증(실어증), 시공간능력 장애, 행동 및 정신행동 증상 등을 보입니다. 진행되면서 불안, 우울, 난폭성 등과 같은 정신증상도 나타날 수 있습니다.알츠하이머병은 치료가 불가능한 질환이지만, 조기에 진단을 받고 적절한 치료와 관리를 시작하는 것이 중요합니다. 이를 통해 삶의 질을 향상시키고 증상을 완화시킬 수 있습니다.\n",
      "수정 후: 알츠하이머병은 노화된 단백질이 뇌세포를 파괴하는 퇴행성 신경 질환입니다. 이 질환은 노화에 따른 뇌신경세포의 죽음으로 인해 발생하며, 정확한 원인은 알려져 있지 않습니다. 초기 증상은 경미한 기억 장애부터 언어 장애, 행동 변화 등으로 나타납니다.알츠하이머병은 점진적인 기억력 감퇴와 언어 장애, 실행증실어증, 시공간능력 장애, 행동 및 정신행동 증상 등을 보입니다. 진행되면서 불안, 우울, 난폭성 등과 같은 정신증상도 나타날 수 있습니다.알츠하이머병은 치료가 불가능한 질환이지만, 조기에 진단을 받고 적절한 치료와 관리를 시작하는 것이 중요합니다. 이를 통해 삶의 질을 향상시키고 증상을 완화시킬 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리 (그런데 이미 불용어 처리된 데이터라 변화가 거의 없음)\n",
    "print(f'수정 전: {question[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", question[20])}')\n",
    "print(f'수정 전: {answer[20]}')\n",
    "print(f'수정 후: {normalizer.sub(\"\", answer[20])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'알츠하이머병의 원인이 30대에서 더 쉽게 찾아볼 수 있는 이유가 있나요?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)\n",
    "\n",
    "normalize(question[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['알츠하이머병',\n",
       " '의',\n",
       " '원인',\n",
       " '이',\n",
       " '30',\n",
       " '대',\n",
       " '에서',\n",
       " '더',\n",
       " '쉽',\n",
       " '게',\n",
       " '찾아볼',\n",
       " '수',\n",
       " '있',\n",
       " '는',\n",
       " '이유',\n",
       " '가',\n",
       " '있',\n",
       " '나요',\n",
       " '?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석 돌려보기\n",
    "mecab = MeCab()\n",
    "mecab.morphs(normalize(question[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 전처리를 함수화\n",
    "def clean_text(sentence, mecab):\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'알츠하이머병 은 노화 된 단백질 이 뇌세포 를 파괴 하 는 퇴행 성 신경 질환 입니다 . 이 질환 은 노화 에 따른 뇌신경 세포 의 죽음 으로 인해 발생 하 며 , 정확 한 원인 은 알려져 있 지 않 습니다 . 초기 증상 은 경미 한 기억 장애 부터 언어 장애 , 행동 변화 등 으로 나타납니다 . 알츠하이머병 은 점진 적 인 기억력 감퇴 와 언어 장애 , 실행증 실어증 , 시공간 능력 장애 , 행동 및 정신 행동 증상 등 을 보입니다 . 진행 되 면서 불안 , 우울 , 난폭 성 등 과 같 은 정신 증상 도 나타날 수 있 습니다 . 알츠하이머병 은 치료 가 불 가능 한 질환 이 지만 , 조기 에 진단 을 받 고 적절 한 치료 와 관리 를 시작 하 는 것 이 중요 합니다 . 이 를 통해 삶 의 질 을 향상 시키 고 증상 을 완화 시킬 수 있 습니다 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글\n",
    "clean_text(question[20], mecab)\n",
    "clean_text(answer[20], mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변을 형태소 분석한 결과를 각각 리스트에 모두 담음\n",
    "questions = [clean_text(sent, mecab) for sent in question.values[:len(question)]]\n",
    "answers = [clean_text(sent, mecab) for sent in answer.values[:len(question)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['알츠하이머병 의 원인 으로 매일 소주 를 섭취 하 는 것 이 언급 되 고 있 는데 , 이 에 대한 근거 가 있 는지 알려 주 세요 .',\n",
       " '알츠하이머병 이 라는 질병 은 유전 적 영향 을 받 는 것 인가요 ?',\n",
       " '알츠하이머병 의 발생 원인 에 대한 연구 나 발견 이 진행 중 인가요 ?',\n",
       " '알츠하이머병 의 발병 과 관련 하 여 뇌 의 노화 로 인한 증상 과 원인 을 알려 주 세요 .',\n",
       " '알츠하이머병 의 원인 과 관련 된 연구 결과 가 있 을까요 ? 알려 주 세요 .']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['알츠하이머병 의 정확 한 원인 은 아직 밝혀 지 지 않 았 지만 , 연구 들 이 알츠하이머병 의 발병 기전 에 대해 논의 하 고 있 습니다 . 일부 연구 에 따르 면 , 유전 적 인 요소 와 뇌 의 기능 손상 이 관련 되 어 있 다고 알려져 있 습니다 . 알츠하이머병 은 아밀로이드 베타 단백질 과 타우 단백질 의 과도 한 생 성 , 뇌 세포 의 비 정상 적 인 활동 , 뇌 조직 의 변화 로 인해 발생 하 는 것 으로 생각 되 고 있 습니다 . 이러 한 변화 가 알츠하이머병 의 발병 위험 을 증가 시키 고 , 병 의 진행 을 가속 화 시킨다는 것 입니다 . 알츠하이머병 의 발병 과 관련 된 위험 요소 에 대해서 는 더 많 은 연구 와 조사 가 필요 합니다 . 더 많 은 연구 와 자료 수집 을 통해 알츠하이머병 에 대한 더 많 은 이해 와 예방 방법 이 개발 될 것 으로 기대 됩니다 .',\n",
       " '알츠하이머병 은 현재 까지 완전 한 원인 이 밝혀 지 지 않 았 습니다 . 알츠하이머병 은 아직 완전히 이해 되 지 않 았 지만 , 연구 결과 에 따르 면 유전 적 인 요소 와 다양 한 환경 적 인 요인 이 이 질환 을 일으키 는 역할 을 한다고 알려져 있 습니다 . 특히 , 아밀로이드 베타 단백질 의 비 정상 적 인 축적 이 알츠하이머병 과 관련 이 있 는 것 으로 알려져 있 습니다 . 이 외 에 도 나이 , 노화 , 고 혈압 , 당뇨병 , 그리고 흡연 등 과 같 은 다른 요인 들 도 알츠하이머병 발병 과 연관 성 이 있 을 수 있 습니다 . 더 많 은 연구 와 조사 를 통해 알츠하이머병 의 원인 을 파악 하 고 예방 방법 을 개발 할 필요 가 있 습니다 .',\n",
       " '알츠하이머병 은 치매 를 일으키 는 가장 흔한 퇴행 성 뇌 질환 으로 , 1907 년 독일 의사 알로이스 알츠하이머 에 의해 처음 으로 보 고 되 었 습니다 . 이 질환 의 원인 에 대해서 는 현재 까지 명확 한 답 은 없 으나 , 치매 발생 의 위험 요소 와 관련 하 여 몇 가지 위험 요인 이 알려져 있 습니다 . 일반 적 으로 , 가장 잘 알려진 요인 중 하나 는 고령 입니다 . 고령 은 치매 의 발병 위험 을 증가 시키 는 가장 큰 위험 요소 로 알려져 있 습니다 . 또한 , 가족력 이 있 는 경우 알츠하이머병 발생 위험 이 높 아 집니다 . 연구 에 따르 면 , 조발 성 가족 성 알츠하이머병 은 주로 65 세 이전 에 발생 하 는 반면 , 노인 성 알츠하이머병 은 주로 65 세 이후 에 발생 한다고 합니다 . 이 외 에 도 , 여성 들 은 65 세 이후 에 더 높 은 위험 에 노출 되 는 경향 이 있 으며 , 우울증 이나 두부 손상 과 같 은 선행 요인 도 알츠하이머병 발병 위험 을 증가 시킬 수 있 습니다 . 또한 , 고 혈압 , 당뇨 , 고지혈증 , 비만 과 같 은 심혈 관 질환 도 알츠하이머병 의 발병 위험 을 증가 시킬 수 있 습니다 . 이 외 에 도 , 다양 한 질병 들 도 알츠하이머병 발병 위험 을 증가 시킬 수 있 습니다 . 하지만 아직 까지 특정 한 원인 에 대해 명확 한 답변 은 없 습니다 . 알츠하이머병 의 정확 한 원인 은 아직 까지 연구 중 이 지만 , 치매 와 관련 된 위험 요소 들 과 함께 유전 적 요소 , 환경 적 요소 들 이 복합 적 으로 작용 하 여 발병 한다는 것 이 유력 한 가설 로 여겨집니다 .',\n",
       " '알츠하이머병 은 현재 까지 그 발병 원인 에 대한 완벽 한 해명 은 아직 이루어지 지 않 았 습니다 . 알츠하이머병 의 발병 기전 은 아직 정확 하 게 밝혀 지 지 않 았 지만 , 유전 적 인 요인 과 다양 한 환경 적 요소 들 이 서로 작용 하 여 발생 한다는 가설 이 제시 되 고 있 습니다 . 대표 적 으로 아밀로이드 베타 단백질 의 쌓임 과 신경 세포 의 손상 이 알츠하이머병 의 발병 원인 으로 알려져 있 으며 , 아밀로이드 베타 단백질 의 침착 이 발생 과 관련 이 있 는 것 으로 추정 됩니다 . 또한 , 알츠하이머병 은 신경 세포 사이 의 신호 전송 과 관련 된 여러 가지 경로 에서 뇌 손상 이 일어나 기 때문 에 다른 뇌 질환 과 도 연관 되 어 있 을 수 있 습니다 . 그러나 이러 한 환경 적 요소 들 과 유전 적 요인 들 은 아직 완전히 규명 되 지 는 않 았 으며 , 개인 의 특성 이나 환경 적 요소 에 따라 발병 가능 성 이 달라질 수 있 습니다 . 알츠하이머병 은 아직 까지 정확 한 원인 이 밝혀 지 지 않 았 지만 , 유전 적 요인 과 다양 한 환경 적 요소 들 이 복합 적 으로 작용 하 여 발병 한다는 점 은 분명 한 사실 입니다 . 추가 적 인 연구 가 필요 하 며 , 예방 및 치료 방법 을 개발 하 기 위해 노력 이 필요 합니다 .',\n",
       " '알츠하이머병 은 복잡 한 질환 으로 , 아직 도 원인 이 완전히 밝혀진 것 은 아닙니다 . 그러나 연구 결과 에 따르 면 , 알츠하이머병 은 단일 원인 으로 발생 하 지 않 고 여러 요인 들 이 복합 적 으로 작용 하 는 것 으로 알려져 있 습니다 . 알츠하이머병 은 21 번 염색체 에 위치 한 21 번 염색체 의 이상 , 14 번 염색체 의 ps 1 유전자 변 이나 19 번 염색체 의 ps 2 유전자 변이 도 주요 원인 으로 보 고 되 고 있 습니다 . 또한 고령 , 다 운증 후군 , 저학력 , 가족력 , 심혈 관 질환 등 다양 한 위험 요인 이 있 습니다 . 더불 어 노화 와 더불 어 콜린 계통 의 활성 화 도 감소 되 는 것 으로 알려져 있 습니다 . 알츠하이머병 은 진행 성 치매 로 , 초기 에 는 주로 기억력 저하 와 인식력 의 감퇴 가 나타납니다 . 그러나 병 이 진행 됨 에 따라 일상 생활 기능 손상 과 판단력 저하 가 더욱 심해집니다 . 치매 환자 는 말 이나 행동 이 느렸 을 뿐 아니 라 최근 기억력 상실 과 언어 장애 가 자주 동반 됩니다 . 수면 이상 , 성격 변화 , 우울증 , 망상 , 환각 , 정신증 등 의 정신 증상 도 나타날 수 있 습니다 . 알츠하이머병 은 아직 정확 한 원인 은 밝혀 지 지 않 았 지만 , 유전 적 인 요소 와 환경 적 인 요인 이 복합 적 으로 작용 하 여 발병 할 것 으로 예상 됩니다 . 유전 적 인 위험 요소 가 있 는 경우 병 이 발현 되 기 쉬울 수 있 지만 , 그렇 지 않 은 경우 에 도 발병 할 가능 성 이 높 습니다 . 알츠하이머병 은 발병 과정 에서 다양 한 위험 요인 들 이 함께 작용 하 는 것 으로 알려져 있 습니다 . 노화 , 다 운증 후군 , 저학력 , 가족력 , 심혈 관 질환 등 이 알츠하이머병 의 위험 요소 로 알려져 있 으며 , 노화 와 더불 어 콜린 계통 의 손상 은 진행 을 가속 화 시킬 수 있 습니다 . 알츠하이머병 이 가장 일찍 인지 되 는 이유 는 최근 일 에 대한 기억력 저하 뿐 아니 라 최근 의 사건 들 에 대한 기억력 까지 저하 된다는 점 입니다 . 다른 정신 증상 으로 는 망상 , 환각 , 불안 , 공격 성 증가 , 우울증 등 이 동반 될 수 있 습니다 . 알츠하이머병 은 아직 정확 한 원인 을 찾 기 어려운 질병 입니다 . 유전 적 인 요소 와 환경 적 인 요인 들 이 상호 작용 하 여 발병 하 는 것 으로 알려져 있 으며 , 다양 한 위험 요인 들 이 서로 연결 되 어 있 습니다 . 이 질병 은 알츠하이머병 이 가장 일찍 인지 되 는 이유 는 최근 일 에 대한 기억력 저하 뿐 아니 라 최근 의 사건 들 에 대한 기억력 까지 저하 된다는 점 입니다 . 따라서 , 알츠하이머병 의 위험 요인 을 최소 화 하 고 건강 한 라이프 스타일 을 유지 하 는 것 이 중요 합니다 . 건강 한 식습관 과 꾸준 한 신체 운동 , 인지 행동 요법 을 통해 삶 의 질 을 개선 하 는 것 이 예방 에 도움 이 됩니다 .']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 모델\n",
    " 1. 질문과 답변 내용을 모두 형태소 분석하여 큰 단어사전을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0 # 빈공간 채워주는 토큰\n",
    "SOS_TOKEN = 1 # 문장의 시작점을 표시하는 토큰\n",
    "EOS_TOKEN = 2 # 문장의 끝을 표시하는 토큰\n",
    "\n",
    "# 단어사전 클래스\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2index = {\n",
    "            '<PAD>': PAD_TOKEN,\n",
    "            '<SOS>': SOS_TOKEN, \n",
    "            '<EOS>': EOS_TOKEN,\n",
    "        }\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_TOKEN: '<PAD>', \n",
    "            SOS_TOKEN: '<SOS>', \n",
    "            EOS_TOKEN: '<EOS>'\n",
    "        }\n",
    "        \n",
    "        self.n_words = 3  # PAD, SOS, EOS 포함\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \n",
    "        # for word in sentence.split(' '):\n",
    "        for word in mecab.morphs(sentence): # 문장을 형태소 분석 함수에 돌리면 각 형태소가 담긴 리스트가 나오기 때문에 바로 반복문으로 하나씩 단어사전에 추가해준다.\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word): # word2index : 단어를 번호로 바꿔주는 딕셔너리, word2count : 해당 단어가 몇번 쓰였는지 횟수를 나타내는 딕셔너리, index2word : 번호를 단어로 바꿔주는 딕셔너리\n",
    "        if word not in self.word2index: # 해당 단어가 단어사전에 없는 경우 번호를 매겨주고, 단어사전에 추가한다.\n",
    "            self.word2index[word] = self.n_words \n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1 \n",
    "        else:\n",
    "            self.word2count[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 질문과 답변을 lang1이란 단어사전 클래스를 만들어서 넣어보기\n",
    "lang1 = WordVocab()\n",
    "for q in question:\n",
    "    lang1.add_sentence(q)\n",
    "for a in answer:\n",
    "    lang1.add_sentence(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '알츠하이머병': 3,\n",
       " '의': 4,\n",
       " '원인': 5,\n",
       " '으로': 6,\n",
       " '매일': 7,\n",
       " '소주': 8,\n",
       " '를': 9,\n",
       " '섭취': 10,\n",
       " '하': 11,\n",
       " '는': 12,\n",
       " '것': 13,\n",
       " '이': 14,\n",
       " '언급': 15,\n",
       " '되': 16,\n",
       " '고': 17,\n",
       " '있': 18,\n",
       " '는데': 19,\n",
       " ',': 20,\n",
       " '에': 21,\n",
       " '대한': 22,\n",
       " '근거': 23,\n",
       " '가': 24,\n",
       " '는지': 25,\n",
       " '알려': 26,\n",
       " '주': 27,\n",
       " '세요': 28,\n",
       " '.': 29,\n",
       " '라는': 30,\n",
       " '질병': 31,\n",
       " '은': 32,\n",
       " '유전': 33,\n",
       " '적': 34,\n",
       " '영향': 35,\n",
       " '을': 36,\n",
       " '받': 37,\n",
       " '인가요': 38,\n",
       " '?': 39,\n",
       " '발생': 40,\n",
       " '연구': 41,\n",
       " '나': 42,\n",
       " '발견': 43,\n",
       " '진행': 44,\n",
       " '중': 45,\n",
       " '발병': 46,\n",
       " '과': 47,\n",
       " '관련': 48,\n",
       " '여': 49,\n",
       " '뇌': 50,\n",
       " '노화': 51,\n",
       " '로': 52,\n",
       " '인한': 53,\n",
       " '증상': 54,\n",
       " '된': 55,\n",
       " '결과': 56,\n",
       " '을까요': 57,\n",
       " '무엇': 58,\n",
       " '주요': 59,\n",
       " '예방': 60,\n",
       " '위해': 61,\n",
       " '한': 62,\n",
       " '병': 63,\n",
       " '씩': 64,\n",
       " '권장': 65,\n",
       " '이유': 66,\n",
       " '다른': 67,\n",
       " '들': 68,\n",
       " '대해': 69,\n",
       " '데': 70,\n",
       " '미치': 71,\n",
       " '요인': 72,\n",
       " '어떤': 73,\n",
       " '나요': 74,\n",
       " '치매': 75,\n",
       " '사이': 76,\n",
       " '어떠': 77,\n",
       " '연관': 78,\n",
       " '성': 79,\n",
       " '하나': 80,\n",
       " '인': 81,\n",
       " '대해서': 82,\n",
       " '자세히': 83,\n",
       " '가족력': 84,\n",
       " '이외': 85,\n",
       " '상세': 86,\n",
       " '게': 87,\n",
       " '설명': 88,\n",
       " '해': 89,\n",
       " '그': 90,\n",
       " '따른': 91,\n",
       " '알': 92,\n",
       " '싶': 93,\n",
       " '어요': 94,\n",
       " '과정': 95,\n",
       " '에서': 96,\n",
       " '가장': 97,\n",
       " '문제': 98,\n",
       " '될': 99,\n",
       " '수': 100,\n",
       " '감염': 101,\n",
       " '균': 102,\n",
       " '정의': 103,\n",
       " '와': 104,\n",
       " '특징': 105,\n",
       " '30': 106,\n",
       " '대': 107,\n",
       " '더': 108,\n",
       " '쉽': 109,\n",
       " '찾아볼': 110,\n",
       " '외': 111,\n",
       " '생기': 112,\n",
       " '현상': 113,\n",
       " '차이점': 114,\n",
       " '알려져': 115,\n",
       " '일어나': 116,\n",
       " '큰': 117,\n",
       " '역할': 118,\n",
       " '마시': 119,\n",
       " '란': 120,\n",
       " '습니다': 121,\n",
       " '확인': 122,\n",
       " '기': 123,\n",
       " '의료': 124,\n",
       " '기관': 125,\n",
       " '방문': 126,\n",
       " '해야': 127,\n",
       " '자세': 128,\n",
       " '듣': 129,\n",
       " '중요': 130,\n",
       " '사항': 131,\n",
       " '흔히': 132,\n",
       " '위한': 133,\n",
       " '조치': 134,\n",
       " '예방법': 135,\n",
       " '나이': 136,\n",
       " '젊': 137,\n",
       " '사람': 138,\n",
       " '에게': 139,\n",
       " '도': 140,\n",
       " '많이': 141,\n",
       " '나타나': 142,\n",
       " '알려진': 143,\n",
       " '존재': 144,\n",
       " '40': 145,\n",
       " '세': 146,\n",
       " '인데': 147,\n",
       " '의해': 148,\n",
       " '할': 149,\n",
       " '일으키': 150,\n",
       " '최신': 151,\n",
       " '요소': 152,\n",
       " '걸리': 153,\n",
       " '이나': 154,\n",
       " '신체': 155,\n",
       " '변화': 156,\n",
       " '주의': 157,\n",
       " '가설': 158,\n",
       " '실': 159,\n",
       " '실험': 160,\n",
       " '진단': 161,\n",
       " '검사': 162,\n",
       " '검토': 163,\n",
       " '할까요': 164,\n",
       " '내': 165,\n",
       " '머리': 166,\n",
       " '속': 167,\n",
       " '지우개': 168,\n",
       " '영화': 169,\n",
       " '나오': 170,\n",
       " '일까요': 171,\n",
       " '위해서': 172,\n",
       " '필요': 173,\n",
       " '한가요': 174,\n",
       " '거나': 175,\n",
       " '관리': 176,\n",
       " '취해야': 177,\n",
       " '인지': 178,\n",
       " '관계': 179,\n",
       " '걸릴': 180,\n",
       " '정보': 181,\n",
       " '퇴행': 182,\n",
       " '최근': 183,\n",
       " '었': 184,\n",
       " '방법': 185,\n",
       " '사용': 186,\n",
       " '추가': 187,\n",
       " '종류': 188,\n",
       " '인자': 189,\n",
       " '효과': 190,\n",
       " '기억': 191,\n",
       " '장애': 192,\n",
       " '언어': 193,\n",
       " '서로': 194,\n",
       " '다른가요': 195,\n",
       " '한다면': 196,\n",
       " '공통점': 197,\n",
       " '정확': 198,\n",
       " '의료진': 199,\n",
       " '제공': 200,\n",
       " '질환': 201,\n",
       " '어': 202,\n",
       " '주장': 203,\n",
       " '점': 204,\n",
       " '관여': 205,\n",
       " '유사': 206,\n",
       " '면서': 207,\n",
       " '건가요': 208,\n",
       " '노인': 209,\n",
       " '에게서': 210,\n",
       " '주로': 211,\n",
       " '작용': 212,\n",
       " '자주': 213,\n",
       " '았': 214,\n",
       " '어떻': 215,\n",
       " '궁금': 216,\n",
       " '합니다': 217,\n",
       " '동향': 218,\n",
       " '좋': 219,\n",
       " '비교': 220,\n",
       " '해서': 221,\n",
       " '수단': 222,\n",
       " '연령': 223,\n",
       " '볼': 224,\n",
       " '이해': 225,\n",
       " '도움': 226,\n",
       " '만': 227,\n",
       " '미칠': 228,\n",
       " '왜': 229,\n",
       " '관한': 230,\n",
       " '기본': 231,\n",
       " '차이': 232,\n",
       " '이란': 233,\n",
       " '의미': 234,\n",
       " '용어': 235,\n",
       " '인해': 236,\n",
       " '핵심': 237,\n",
       " '파킨슨병': 238,\n",
       " '가지': 239,\n",
       " '특성': 240,\n",
       " '우리': 241,\n",
       " '아': 242,\n",
       " '그리고': 243,\n",
       " '어떻게': 244,\n",
       " '정확히': 245,\n",
       " '기준': 246,\n",
       " '같': 247,\n",
       " '갖': 248,\n",
       " '신경': 249,\n",
       " '면': 250,\n",
       " '구분': 251,\n",
       " '다른지': 252,\n",
       " '명칭': 253,\n",
       " '치료': 254,\n",
       " '보이': 255,\n",
       " '구별': 256,\n",
       " '될까요': 257,\n",
       " '노망': 258,\n",
       " '병인': 259,\n",
       " '경우': 260,\n",
       " '말': 261,\n",
       " '나타날': 262,\n",
       " '비슷': 263,\n",
       " '라고': 264,\n",
       " '식': 265,\n",
       " '동일': 266,\n",
       " '방식': 267,\n",
       " '증세': 268,\n",
       " '맞': 269,\n",
       " '유발': 270,\n",
       " '형태': 271,\n",
       " '내용': 272,\n",
       " '기억력': 273,\n",
       " '감퇴': 274,\n",
       " '단어': 275,\n",
       " '잘': 276,\n",
       " '떠오르': 277,\n",
       " '지': 278,\n",
       " '않': 279,\n",
       " '일상': 280,\n",
       " '생활': 281,\n",
       " '환자': 282,\n",
       " '특히': 283,\n",
       " '대표': 284,\n",
       " '심각': 285,\n",
       " '다면': 286,\n",
       " '초기': 287,\n",
       " '단계': 288,\n",
       " '상': 289,\n",
       " '실증': 290,\n",
       " '아니': 291,\n",
       " '라': 292,\n",
       " '걸렸': 293,\n",
       " '때': 294,\n",
       " '일반인': 295,\n",
       " '흔한': 296,\n",
       " '얼마나': 297,\n",
       " '시간': 298,\n",
       " '가까운': 299,\n",
       " '저하': 300,\n",
       " '일반': 301,\n",
       " '시': 302,\n",
       " '할머니': 303,\n",
       " '동반': 304,\n",
       " '조심': 305,\n",
       " '상실': 306,\n",
       " '잊어버리': 307,\n",
       " '포함': 308,\n",
       " '먼저': 309,\n",
       " '느끼': 310,\n",
       " '생각': 311,\n",
       " '못하': 312,\n",
       " '잃': 313,\n",
       " '해요': 314,\n",
       " '나타날까요': 315,\n",
       " '나타내': 316,\n",
       " '시설': 317,\n",
       " '어디': 318,\n",
       " '상세히': 319,\n",
       " '각각': 320,\n",
       " '절차': 321,\n",
       " '거쳐야': 322,\n",
       " '어야': 323,\n",
       " '느낄': 324,\n",
       " '으려면': 325,\n",
       " '아야': 326,\n",
       " '의한': 327,\n",
       " '함께': 328,\n",
       " '예후': 329,\n",
       " '병원': 330,\n",
       " '의사': 331,\n",
       " '찾': 332,\n",
       " '이루': 333,\n",
       " '약화': 334,\n",
       " '판단': 335,\n",
       " '특이': 336,\n",
       " '부탁드립니다': 337,\n",
       " '경험': 338,\n",
       " '쉬운': 339,\n",
       " '시행': 340,\n",
       " '전문가': 341,\n",
       " '누구': 342,\n",
       " '보통': 343,\n",
       " '이상': 344,\n",
       " '떠올리': 345,\n",
       " '혈액': 346,\n",
       " '중증': 347,\n",
       " '정하': 348,\n",
       " '알츠하이머': 349,\n",
       " 'Apo': 350,\n",
       " 'E': 351,\n",
       " '유전자': 352,\n",
       " '어려움': 353,\n",
       " '조사': 354,\n",
       " '의학': 355,\n",
       " '증도': 356,\n",
       " '전문의': 357,\n",
       " '길': 358,\n",
       " '집': 359,\n",
       " '못': 360,\n",
       " '려면': 361,\n",
       " '찾아가': 362,\n",
       " '야': 363,\n",
       " '기능': 364,\n",
       " '이름': 365,\n",
       " '연락처': 366,\n",
       " '곳': 367,\n",
       " '검색어': 368,\n",
       " '어서': 369,\n",
       " '문진': 370,\n",
       " 'CT': 371,\n",
       " '줄': 372,\n",
       " '상담': 373,\n",
       " '원리': 374,\n",
       " '의심': 375,\n",
       " '아버지': 376,\n",
       " '버리': 377,\n",
       " '적절': 378,\n",
       " '찍': 379,\n",
       " '가능': 380,\n",
       " '한지': 381,\n",
       " '평가': 382,\n",
       " '영상': 383,\n",
       " '목적': 384,\n",
       " '통해': 385,\n",
       " '보건소': 386,\n",
       " '소개': 387,\n",
       " '사진': 388,\n",
       " '필수': 389,\n",
       " '진료': 390,\n",
       " '전문': 391,\n",
       " '분야': 392,\n",
       " '어느': 393,\n",
       " '무료': 394,\n",
       " '내원': 395,\n",
       " '아니면': 396,\n",
       " '자기': 397,\n",
       " '공명': 398,\n",
       " '순서': 399,\n",
       " '이루어지': 400,\n",
       " '따라야': 401,\n",
       " '비용': 402,\n",
       " '정도': 403,\n",
       " '기록': 404,\n",
       " '활용': 405,\n",
       " '걸릴까요': 406,\n",
       " '나타났': 407,\n",
       " '조건': 408,\n",
       " '수집': 409,\n",
       " '으면': 410,\n",
       " '보': 411,\n",
       " '메커니즘': 412,\n",
       " '약물': 413,\n",
       " '치료법': 414,\n",
       " '개인': 415,\n",
       " '노력': 416,\n",
       " '근본': 417,\n",
       " '아직': 418,\n",
       " '까지': 419,\n",
       " '없': 420,\n",
       " '목표': 421,\n",
       " '며': 422,\n",
       " '보편': 423,\n",
       " '수술': 424,\n",
       " '시술': 425,\n",
       " '완치': 426,\n",
       " '현재': 427,\n",
       " '줄이': 428,\n",
       " '가족': 429,\n",
       " '소식': 430,\n",
       " '습관': 431,\n",
       " '간헐': 432,\n",
       " '단식': 433,\n",
       " '다양': 434,\n",
       " '후': 435,\n",
       " '예상': 436,\n",
       " '부작용': 437,\n",
       " '보조제': 438,\n",
       " '어떤지': 439,\n",
       " '대안': 440,\n",
       " '서비스': 441,\n",
       " '기대': 442,\n",
       " '개발': 443,\n",
       " '개선': 444,\n",
       " '협력': 445,\n",
       " '다고': 446,\n",
       " '언제': 447,\n",
       " '쯤': 448,\n",
       " '지원': 449,\n",
       " '자연': 450,\n",
       " '요법': 451,\n",
       " '보조': 452,\n",
       " '통제': 453,\n",
       " '전략': 454,\n",
       " '기술': 455,\n",
       " '분': 456,\n",
       " '기간': 457,\n",
       " '접근': 458,\n",
       " '시도': 459,\n",
       " '실천': 460,\n",
       " '재활': 461,\n",
       " '완전히': 462,\n",
       " '다는데': 463,\n",
       " '그런가요': 464,\n",
       " '완화': 465,\n",
       " '시키': 466,\n",
       " '성공률': 467,\n",
       " '식단': 468,\n",
       " '조절': 469,\n",
       " '많': 470,\n",
       " '식이': 471,\n",
       " '추천': 472,\n",
       " '법': 473,\n",
       " '앓': 474,\n",
       " '알코올': 475,\n",
       " '검진': 476,\n",
       " '해석': 477,\n",
       " '센터': 478,\n",
       " '검색': 479,\n",
       " '장소': 480,\n",
       " '잠': 481,\n",
       " '자': 482,\n",
       " '알콜': 483,\n",
       " '테스트': 484,\n",
       " '검': 485,\n",
       " '진': 486,\n",
       " '학과': 487,\n",
       " '소견': 488,\n",
       " '가진': 489,\n",
       " '간단': 490,\n",
       " '서': 491,\n",
       " '유형': 492,\n",
       " '할지': 493,\n",
       " '조언': 494,\n",
       " '술': 495,\n",
       " '예약': 496,\n",
       " '의원': 497,\n",
       " '수면': 498,\n",
       " '심해': 499,\n",
       " '이용': 500,\n",
       " '클리닉': 501,\n",
       " '위치': 502,\n",
       " '조기': 503,\n",
       " '집중력': 504,\n",
       " '떨어지': 505,\n",
       " '유명': 506,\n",
       " '얻': 507,\n",
       " '음주': 508,\n",
       " '잦': 509,\n",
       " '치': 510,\n",
       " '거치': 511,\n",
       " '소요': 512,\n",
       " '전문과': 513,\n",
       " '정확도': 514,\n",
       " '보여': 515,\n",
       " '계신가요': 516,\n",
       " '과다': 517,\n",
       " '요': 518,\n",
       " '발병률': 519,\n",
       " '높일': 520,\n",
       " '꼭': 521,\n",
       " '문의': 522,\n",
       " '적합': 523,\n",
       " '입력': 524,\n",
       " '믿': 525,\n",
       " '없이': 526,\n",
       " '프로그램': 527,\n",
       " '실시': 528,\n",
       " '참고': 529,\n",
       " '오랜': 530,\n",
       " '동안': 531,\n",
       " '특별히': 532,\n",
       " '행동': 533,\n",
       " '과음': 534,\n",
       " '금주': 535,\n",
       " '위험': 536,\n",
       " '최소': 537,\n",
       " '화': 538,\n",
       " '운동': 539,\n",
       " '건강': 540,\n",
       " '식습관': 541,\n",
       " '식품': 542,\n",
       " '영양소': 543,\n",
       " '끊': 544,\n",
       " '가치': 545,\n",
       " '영양가': 546,\n",
       " '음식': 547,\n",
       " '유지': 548,\n",
       " '자제': 549,\n",
       " '취할': 550,\n",
       " '은데': 551,\n",
       " '복용': 552,\n",
       " '마실': 553,\n",
       " '이점': 554,\n",
       " '가져야': 555,\n",
       " '양': 556,\n",
       " '빈도': 557,\n",
       " '접종': 558,\n",
       " '줄일': 559,\n",
       " '음': 560,\n",
       " '주량': 561,\n",
       " '장점': 562,\n",
       " '식사': 563,\n",
       " '전': 564,\n",
       " '적용': 565,\n",
       " '먹': 566,\n",
       " '권고': 567,\n",
       " '환경': 568,\n",
       " '스트레스': 569,\n",
       " '방지': 570,\n",
       " '지켜야': 571,\n",
       " '규칙': 572,\n",
       " '써야': 573,\n",
       " '피하': 574,\n",
       " '금단': 575,\n",
       " '맥주': 576,\n",
       " '예방책': 577,\n",
       " '가질': 578,\n",
       " '도록': 579,\n",
       " '영양제': 580,\n",
       " '마셨': 581,\n",
       " '필름': 582,\n",
       " '끊기': 583,\n",
       " '섭': 584,\n",
       " '취량': 585,\n",
       " '조금': 586,\n",
       " '횟수': 587,\n",
       " '차지': 588,\n",
       " '비율': 589,\n",
       " '높': 590,\n",
       " '은가요': 591,\n",
       " '중독': 592,\n",
       " '을수록': 593,\n",
       " '는지요': 594,\n",
       " '소비': 595,\n",
       " '간': 596,\n",
       " '기여': 597,\n",
       " '이론': 598,\n",
       " '할아버지': 599,\n",
       " '따라': 600,\n",
       " '달라지': 601,\n",
       " '사실': 602,\n",
       " '으로서': 603,\n",
       " '다는': 604,\n",
       " '걸린': 605,\n",
       " '인과': 606,\n",
       " '늦': 607,\n",
       " '악화': 608,\n",
       " '상관': 609,\n",
       " '부분': 610,\n",
       " '주가': 611,\n",
       " '혈관': 612,\n",
       " '상태': 613,\n",
       " '가리키': 614,\n",
       " '의존': 615,\n",
       " '상황': 616,\n",
       " '구체': 617,\n",
       " '그렇': 618,\n",
       " '소비량': 619,\n",
       " '증이': 620,\n",
       " '그것': 621,\n",
       " '두드러지': 622,\n",
       " '관해서': 623,\n",
       " '처음': 624,\n",
       " '경미': 625,\n",
       " '수준': 626,\n",
       " '다가': 627,\n",
       " '점차': 628,\n",
       " '깜박거리': 629,\n",
       " '겪': 630,\n",
       " '블랙아웃': 631,\n",
       " '깜빡': 632,\n",
       " '전날': 633,\n",
       " '흐릿': 634,\n",
       " '평소': 635,\n",
       " '즉시': 636,\n",
       " '중간': 637,\n",
       " '까먹': 638,\n",
       " '지속': 639,\n",
       " '마셔서': 640,\n",
       " '평상시': 641,\n",
       " '표현': 642,\n",
       " '흐려': 643,\n",
       " '정신': 644,\n",
       " '계속': 645,\n",
       " '일': 646,\n",
       " '해당': 647,\n",
       " '깜박': 648,\n",
       " '깜빡하': 649,\n",
       " '지남': 650,\n",
       " '깜빡거리': 651,\n",
       " '대처': 652,\n",
       " '(': 653,\n",
       " ')': 654,\n",
       " '장기간': 655,\n",
       " '가요': 656,\n",
       " '종종': 657,\n",
       " '흐려질': 658,\n",
       " '깜빡깜빡': 659,\n",
       " '볼까요': 660,\n",
       " '흔하': 661,\n",
       " '시킬': 662,\n",
       " '점점': 663,\n",
       " '해질': 664,\n",
       " '경감': 665,\n",
       " '다시': 666,\n",
       " '거쳐서': 667,\n",
       " '고려': 668,\n",
       " '기기': 669,\n",
       " '도구': 670,\n",
       " '준비': 671,\n",
       " '재': 672,\n",
       " '관찰': 673,\n",
       " '마신': 674,\n",
       " '남성': 675,\n",
       " '으셨다면': 676,\n",
       " '질문': 677,\n",
       " '스스로': 678,\n",
       " '안': 679,\n",
       " '뚜렷': 680,\n",
       " '따르': 681,\n",
       " '자료': 682,\n",
       " '격리': 683,\n",
       " '치유': 684,\n",
       " '바꾸': 685,\n",
       " '해결': 686,\n",
       " '보호': 687,\n",
       " '장비': 688,\n",
       " '선택': 689,\n",
       " '다루': 690,\n",
       " '반드시': 691,\n",
       " '높이': 692,\n",
       " '과도': 693,\n",
       " '회복': 694,\n",
       " '취하': 695,\n",
       " '반응': 696,\n",
       " '옵션': 697,\n",
       " '성공': 698,\n",
       " '률': 699,\n",
       " '약': 700,\n",
       " '제한': 701,\n",
       " '대체': 702,\n",
       " '조성': 703,\n",
       " '바꿔야': 704,\n",
       " '합병증': 705,\n",
       " '우울': 706,\n",
       " '감정': 707,\n",
       " '우울증': 708,\n",
       " '걱정': 709,\n",
       " '없앨': 710,\n",
       " '산후': 711,\n",
       " '처방': 712,\n",
       " '세로토닌': 713,\n",
       " '재흡수': 714,\n",
       " '억제제': 715,\n",
       " '빨리': 716,\n",
       " '호전': 717,\n",
       " '해도': 718,\n",
       " '오래': 719,\n",
       " '푸': 720,\n",
       " '록': 721,\n",
       " '틴': 722,\n",
       " '불구': 723,\n",
       " '이러': 724,\n",
       " '두': 725,\n",
       " '동시': 726,\n",
       " '작': 727,\n",
       " '질수록': 728,\n",
       " '빠른': 729,\n",
       " '피해야': 730,\n",
       " '통한': 731,\n",
       " '가져다주': 732,\n",
       " '거부': 733,\n",
       " '복용량': 734,\n",
       " '멍해': 735,\n",
       " '꾸준히': 736,\n",
       " '어도': 737,\n",
       " '기분': 738,\n",
       " '아침': 739,\n",
       " 'adhd': 740,\n",
       " '모유': 741,\n",
       " '수유': 742,\n",
       " '몸': 743,\n",
       " '는다면': 744,\n",
       " '임의': 745,\n",
       " '중단': 746,\n",
       " '은지': 747,\n",
       " '안전': 748,\n",
       " '아기': 749,\n",
       " '부정': 750,\n",
       " '공황': 751,\n",
       " '성분': 752,\n",
       " '두통약': 753,\n",
       " '증가': 754,\n",
       " '꾸준': 755,\n",
       " '사라질': 756,\n",
       " '일시': 757,\n",
       " '멈출': 758,\n",
       " '제': 759,\n",
       " '마음대로': 760,\n",
       " '자신': 761,\n",
       " '감': 762,\n",
       " '능력': 763,\n",
       " '향상': 764,\n",
       " '고통': 765,\n",
       " '경구': 766,\n",
       " '용': 767,\n",
       " '말기': 768,\n",
       " '신': 769,\n",
       " '질물': 770,\n",
       " '더라도': 771,\n",
       " '생길': 772,\n",
       " '뇌전증': 773,\n",
       " '미칠까요': 774,\n",
       " '오랫동안': 775,\n",
       " '했': 776,\n",
       " '해소': 777,\n",
       " '항우울제': 778,\n",
       " '알프라졸람': 779,\n",
       " '나빠': 780,\n",
       " '입증': 781,\n",
       " '출산': 782,\n",
       " '혜택': 783,\n",
       " '극복': 784,\n",
       " '장기': 785,\n",
       " '호흡': 786,\n",
       " '증후군': 787,\n",
       " '중지': 788,\n",
       " '괜찮': 789,\n",
       " '걸까요': 790,\n",
       " '막': 791,\n",
       " '옳': 792,\n",
       " '겠': 793,\n",
       " '쓰이': 794,\n",
       " '약효': 795,\n",
       " '야기': 796,\n",
       " '없애': 797,\n",
       " '편': 798,\n",
       " '더뎌': 799,\n",
       " '심해질': 800,\n",
       " '치료제': 801,\n",
       " '진다면': 802,\n",
       " '사라지': 803,\n",
       " '질': 804,\n",
       " '사회': 805,\n",
       " '어려운': 806,\n",
       " '특화': 807,\n",
       " '가져올까요': 808,\n",
       " '조합': 809,\n",
       " '빠르': 810,\n",
       " '증약': 811,\n",
       " '상호': 812,\n",
       " '어려울': 813,\n",
       " '까요': 814,\n",
       " '후유증': 815,\n",
       " '남': 816,\n",
       " '비해': 817,\n",
       " '가져올': 818,\n",
       " '감소': 819,\n",
       " '같이': 820,\n",
       " '일으킬': 821,\n",
       " '미미': 822,\n",
       " '강화': 823,\n",
       " '으면서': 824,\n",
       " '통': 825,\n",
       " '아도': 826,\n",
       " '정서': 827,\n",
       " '발달': 828,\n",
       " '긴': 829,\n",
       " '또는': 830,\n",
       " '사례': 831,\n",
       " '부담': 832,\n",
       " '우려': 833,\n",
       " '시기': 834,\n",
       " '정상': 835,\n",
       " '완쾌': 836,\n",
       " '체중': 837,\n",
       " '줄어들': 838,\n",
       " '제대로': 839,\n",
       " '및': 840,\n",
       " '재발': 841,\n",
       " '된다면': 842,\n",
       " '초래': 843,\n",
       " '영양': 844,\n",
       " '낮': 845,\n",
       " '그러나': 846,\n",
       " '계시': 847,\n",
       " '아닌가요': 848,\n",
       " '유일': 849,\n",
       " '해결책': 850,\n",
       " '갱년기': 851,\n",
       " '자외선': 852,\n",
       " '노출': 853,\n",
       " '피부암': 854,\n",
       " '등': 855,\n",
       " '골다공증': 856,\n",
       " '햇볕': 857,\n",
       " '쬐': 858,\n",
       " '자살': 859,\n",
       " '정신과': 860,\n",
       " '조현': 861,\n",
       " '이어지': 862,\n",
       " '요즘': 863,\n",
       " '외롭': 864,\n",
       " '느낀다면': 865,\n",
       " '외로움': 866,\n",
       " '영향력': 867,\n",
       " '큰지': 868,\n",
       " '만사': 869,\n",
       " '욕': 870,\n",
       " '된다는': 871,\n",
       " '심리': 872,\n",
       " '진다는데': 873,\n",
       " '처럼': 874,\n",
       " '활기차': 875,\n",
       " '힘들': 876,\n",
       " '느껴': 877,\n",
       " '외부': 878,\n",
       " '이후': 879,\n",
       " '연결': 880,\n",
       " '고리': 881,\n",
       " '주된': 882,\n",
       " '이어질': 883,\n",
       " '여야': 884,\n",
       " '현황': 885,\n",
       " '이런': 886,\n",
       " '느껴진다면': 887,\n",
       " '아이': 888,\n",
       " '낳': 889,\n",
       " '내부': 890,\n",
       " '소': 891,\n",
       " '오메가': 892,\n",
       " '3': 893,\n",
       " '지방산': 894,\n",
       " '여성': 895,\n",
       " '폐경': 896,\n",
       " '긍정': 897,\n",
       " '학교': 898,\n",
       " '싫': 899,\n",
       " '음증': 900,\n",
       " '삶': 901,\n",
       " '멍한': 902,\n",
       " '증과': 903,\n",
       " '폐경기': 904,\n",
       " '소아': 905,\n",
       " '과잉': 906,\n",
       " '기복': 907,\n",
       " '충동': 908,\n",
       " '불안': 909,\n",
       " '웃': 910,\n",
       " '으려고': 911,\n",
       " '얼굴': 912,\n",
       " '일부': 913,\n",
       " '산모': 914,\n",
       " '산욕기': 915,\n",
       " '불안감': 916,\n",
       " '불': 917,\n",
       " '죄책감': 918,\n",
       " '느낌': 919,\n",
       " '바닥': 920,\n",
       " '내려가': 921,\n",
       " '호로': 922,\n",
       " '몬': 923,\n",
       " '무기력감': 924,\n",
       " '징후': 925,\n",
       " '상실감': 926,\n",
       " '무력감': 927,\n",
       " '분류': 928,\n",
       " '표정': 929,\n",
       " '변동': 930,\n",
       " '일어날': 931,\n",
       " '드': 932,\n",
       " '성향': 933,\n",
       " '무': 934,\n",
       " '기력': 935,\n",
       " '눈물': 936,\n",
       " '전형': 937,\n",
       " '이것': 938,\n",
       " '안정': 939,\n",
       " '생겨': 940,\n",
       " '살': 941,\n",
       " '갑자기': 942,\n",
       " '피로감': 943,\n",
       " '헌팅': 944,\n",
       " '톤': 945,\n",
       " '병과': 946,\n",
       " '분만': 947,\n",
       " '일치': 948,\n",
       " '딸': 949,\n",
       " '울음': 950,\n",
       " '소리': 951,\n",
       " '짜증': 952,\n",
       " '시달리': 953,\n",
       " '성욕': 954,\n",
       " '복부비만': 955,\n",
       " '함': 956,\n",
       " '6': 957,\n",
       " '개월': 958,\n",
       " '창문': 959,\n",
       " '울': 960,\n",
       " '식은땀': 961,\n",
       " '열감': 962,\n",
       " '을지': 963,\n",
       " '다를까요': 964,\n",
       " '가을': 965,\n",
       " '무기력증': 966,\n",
       " '심하': 967,\n",
       " '나타난': 968,\n",
       " '임신': 969,\n",
       " '차': 970,\n",
       " '식욕': 971,\n",
       " '으며': 972,\n",
       " '오': 973,\n",
       " '하루': 974,\n",
       " '몇': 975,\n",
       " '번': 976,\n",
       " '마음': 977,\n",
       " '허탈감': 978,\n",
       " '월경': 979,\n",
       " '세상': 980,\n",
       " '사': 981,\n",
       " '재미': 982,\n",
       " '긴장': 983,\n",
       " '초조': 984,\n",
       " '늘': 985,\n",
       " '피곤': 986,\n",
       " '졸리': 987,\n",
       " '느껴질': 988,\n",
       " '갑작스러운': 989,\n",
       " '슬픔': 990,\n",
       " '뒤': 991,\n",
       " '자꾸': 992,\n",
       " '혹시': 993,\n",
       " '아닌지': 994,\n",
       " '지난': 995,\n",
       " '슬픈': 996,\n",
       " '허탈': 997,\n",
       " '무기력': 998,\n",
       " '지연': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: 알츠하이머병 진단 을 위해 뇌 영상 검사 의 필요 성 과 목적 에 대해 알려 주 세요 .\n",
      "==============================\n",
      "[단어사전]\n",
      "******************************\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '알츠하이머병': 3, '진단': 4, '을': 5, '위해': 6, '뇌': 7, '영상': 8, '검사': 9, '의': 10, '필요': 11, '성': 12, '과': 13, '목적': 14, '에': 15, '대해': 16, '알려': 17, '주': 18, '세요': 19, '.': 20}\n"
     ]
    }
   ],
   "source": [
    "# 문장 하나 가져와서 단어사전에 추가해서 확인 (테스트용 코드)\n",
    "print(f'원문: {questions[550]}')\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(questions[550])\n",
    "print('==='*10)\n",
    "print('[단어사전]')\n",
    "print('***'*10)\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: [90, 86, 54, 32, 42, 12, 30, 79, 35, 26, 37, 46, 21, 77, 46, 74, 40, 5, 5, 43, 90, 54, 52, 88, 70, 24, 91, 91, 27, 33]\n",
      "Output: [90, 86, 54, 32, 42, 12, 30, 79, 35, 26, 37, 46, 21, 77, 46, 74, 40, 5, 5, 43, 90, 54, 52, 88, 70, 24, 91, 91, 27, 33, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total Length: 50\n"
     ]
    }
   ],
   "source": [
    "# 문장 생성 테스트(나중에 데이터셋 만들때 필요한 과정)\n",
    "\n",
    "max_length = 50 # 문장을 담을 길이를 정하기\n",
    "sentence_length = 30 # 입력할 문장길이\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,)) # 랜덤으로 단어 뽑아오기(3번~100까지)\n",
    "sentence_tokens = sentence_tokens.tolist() \n",
    "print(f'Generated Sentence: {sentence_tokens}')\n",
    "\n",
    "sentence_tokens = sentence_tokens[:(max_length-1)]\n",
    "\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "# 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "sentence_tokens.append(2)\n",
    "\n",
    "for i in range(token_length, max_length-1):\n",
    "    # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "    sentence_tokens.append(0)\n",
    "\n",
    "print(f'Output: {sentence_tokens}')\n",
    "print(f'Total Length: {len(sentence_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 데이터 만들기(여기가 중요!)\n",
    " 1. csv파일에서 질문, 답변 데이터를 읽어와 데이터 프레임을 만듬\n",
    " 2. 질문, 답변내용에서 형태소 분석(mecab 사용)을 이용해 단어사전을 만듬\n",
    " 3. 질문내용은 30개 토큰, 답변 내용은 300개 토큰을 사용 (q_max_length, a_max_length로 조절할 수 있음)\n",
    " 4. 입력 문장을 단어사전을 이용해 숫자로 변환한 후 리스트로 만듬 ex) \"치매에 좋은 운동은 뭐가 있나요?\" -> [치매에, 좋은, 운동, 은, 뭐가, 있나요?] -> [2, 4, 5, 6, 7, 8, 0.....,0](길이가 30인 숫자 리스트로 변환)\n",
    " 5. 답변 문장도 같은 형식으로 길이가 300인 리스트로 변환\n",
    " 6. 의도는 각 단어마다 번호를 매겨 하나의 숫자로 변환\n",
    " 7. 출력은 {\"answer\" : 답변내용문자열, \"intention\" : 의도에 해당하는 숫자} 형태의 딕셔너리로 만들었음\n",
    " 8. 그리고 각 input값과 출력값은 학습시키려면 tensor로 변환시켜줘야하기 때문에, torch.tensor()함수로 감싸서 텐서형태로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, min_length=3, max_length1=50, q_max_length=30, a_max_length=300):\n",
    "        super(TextDataset, self).__init__()\n",
    "        # data_dir = 'data'\n",
    "        \n",
    "        # TOKEN 정의\n",
    "        self.PAD_TOKEN = 0 # Padding 토큰\n",
    "        self.SOS_TOKEN = 1 # SOS 토큰\n",
    "        self.EOS_TOKEN = 2 # EOS 토큰\n",
    "        \n",
    "        self.tagger = MeCab()   # 형태소 분석기\n",
    "        self.max_length1 = max_length # 한 문장의 최대 길이 지정\n",
    "        self.q_max_length = q_max_length # 질문 길이 최대 지정\n",
    "        self.a_max_length = a_max_length # 답변 길이 최대 지정\n",
    "        \n",
    "        # CSV 데이터 로드\n",
    "        # df = pd.read_csv(os.path.join(data_dir, csv_path))\n",
    "        df=pd.read_csv('dementia_fix.csv') # 질문, 답변, 의도가 저장된 csv파일\n",
    "        # 한글 정규화\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "        \n",
    "        # src: 질의, itn: 의도 tgt: 답변\n",
    "        src_clean = []\n",
    "        itn_clean = [] \n",
    "        tgt_clean = []\n",
    "        \n",
    "        # 단어 사전 생성\n",
    "        wordvocab = WordVocab()\n",
    "        itn_label = {\"검진\" : 0, \"식이, 생활\" : 1, \"약물\" : 2, \"예방\" : 3, \"운동\" : 4, \"원인\" : 5, \"정의\" : 6, \"증상\" : 7, \"진단\" : 8, \"치료\" : 9}\n",
    "        for _, row in df.iterrows():\n",
    "            src = row['question']\n",
    "            itn = row['intention']\n",
    "            tgt = row['answer']\n",
    "            \n",
    "            # 한글 전처리\n",
    "            src = self.clean_text(src)\n",
    "            tgt = self.clean_text(tgt)\n",
    "            \n",
    "            if len(src.split()) > min_length and len(tgt.split()) > min_length:\n",
    "                # 최소 길이를 넘어가는 문장의 단어만 추가\n",
    "                wordvocab.add_sentence(src)\n",
    "                wordvocab.add_sentence(tgt)\n",
    "                src_clean.append(src)          \n",
    "                tgt_clean.append(tgt)\n",
    "            itn_clean.append(itn_label[itn])\n",
    "        \n",
    "        self.srcs = src_clean\n",
    "        self.itns = itn_clean\n",
    "        self.tgts = tgt_clean\n",
    "        self.wordvocab = wordvocab\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        # 정규표현식에 따른 한글 정규화\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        # 한글 정규화\n",
    "        sentence = self.normalize(sentence)\n",
    "        # 형태소 처리\n",
    "        sentence = self.tagger.morphs(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "    \n",
    "    def texts_to_sequences(self, sentence):\n",
    "        # 문장 -> 시퀀스로 변환\n",
    "        return [self.wordvocab.word2index[w] for w in mecab.morphs(sentence)]\n",
    "    \n",
    "        # return [self.wordvocab.word2index[w] for w in sentence.split()]\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens, max_length):\n",
    "        # 문장의 맨 끝 토큰은 제거\n",
    "        sentence_tokens = sentence_tokens[:(max_length-1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "\n",
    "        # 문장의 맨 끝부분에 <EOS> 토큰 추가\n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "\n",
    "        for i in range(token_length, (max_length-1)):\n",
    "            # 나머지 빈 곳에 <PAD> 토큰 추가\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "        return sentence_tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터프레임 구조\n",
    "        #------------------------------------\n",
    "        # 1 | 질문내용 | 의도 | 답변내용 \n",
    "        # 2 | 질문내용 | 의도 | 답변내용 \n",
    "        # ...\n",
    "        # 마지막 idx | 질문내용 | 의도 | 답변내용 \n",
    "        #-------------------------------------\n",
    "        # 여기서 한줄씩 뽑아서 학습용 데이터셋을 구성\n",
    "        # inputs = self.srcs[idx]\n",
    "        \n",
    "        # 입력형태 만들기\n",
    "        inputs_sequences = self.texts_to_sequences(self.srcs[idx])\n",
    "        inputs_padded = self.pad_sequence(inputs_sequences, self.q_max_length)\n",
    "        \n",
    "        # outputs = self.tgts[idx]\n",
    "        # 출력 형태 만들기\n",
    "        outputs = {}\n",
    "        outputs_sequences = self.texts_to_sequences(self.tgts[idx])\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences, self.a_max_length)\n",
    "        outputs['answer'] = torch.tensor(outputs_padded)\n",
    "        outputs['intention'] = torch.tensor(self.itns[idx])\n",
    "        \n",
    "        return torch.tensor(inputs_padded), outputs\n",
    "        # return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장의 최대 단어길이를 300로 설정\n",
    "MAX_LENGTH = 300 # 안써도 되는 파라미터\n",
    "Q_MAX_LENGTH = 30 # 질문 문장 최대 30개 토큰 사용\n",
    "A_MAX_LENGTH = 350 # 답변 문장 최대 350개 토큰 사용\n",
    "\n",
    "dataset = TextDataset('dementia_fix.csv', min_length=3, max_length1=MAX_LENGTH, q_max_length=Q_MAX_LENGTH, a_max_length=A_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5017"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어사전 등록된 단어 개수\n",
    "dataset.wordvocab.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "         14, 21, 22, 23, 24, 18, 25, 26, 27, 28, 29,  2]),\n",
       " {'answer': tensor([  3,   4,  30,  31,   5,  32,  33,  34,  35,  35,  36,  37,  38,  20,\n",
       "           39,  40,  14,   3,   4,  41,  42,  21,  43,  44,  11,  17,  18,  45,\n",
       "           29,  46,  39,  21,  47,  48,  20,  49,  50,  51,  52,  53,  54,   4,\n",
       "           55,  56,  14,  57,  16,  58,  18,  59,  60,  18,  45,  29,   3,  32,\n",
       "           61,  62,  63,  64,  65,  63,   4,  64,  66,  31,  67,  68,  20,  54,\n",
       "           69,   4,  70,  71,  50,  51,  72,  20,  54,  73,   4,  74,  75,  76,\n",
       "           77,  11,  12,  13,   6,  78,  16,  17,  18,  45,  29,  79,  31,  74,\n",
       "           24,   3,   4,  41,  80,  81,  82,  83,  17,  20,  84,   4,  85,  81,\n",
       "           86,  87,  88,  13,  89,  29,   3,   4,  41,  64,  57,  90,  80,  52,\n",
       "           21,  91,  12,  92,  93,  32,  39,  53,  94,  24,  95,  96,  29,  92,\n",
       "           93,  32,  39,  53,  97,  98,  81,  99,   3,  21,  22,  92,  93,  32,\n",
       "          100,  53, 101, 102,  14, 103, 104,  13,   6, 105, 106,  29,   2,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       "  'intention': tensor(5)})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋으로 변환한 결과 확인\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 데이터 임의 추출\n",
    "x, y = dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3,  14,  77,  11,  12, 405,  21,  12, 410, 121,  40,  14, 109,  81,\n",
       "        360, 422, 112,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([30])\n",
      "tensor([  3,  14,  77,  11,  12, 405,  21,  12, 410, 121,  40,  14, 109,  81,\n",
      "        360, 422, 112,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "y shape: torch.Size([350])\n",
      "{'answer': tensor([  3,  32, 113, 114, 115,  31,   5,  81, 346, 139, 161,  12, 108,  89,\n",
      "         29, 201, 164, 165, 138,  90,  49,  50, 121,  40,  14,   3,   4,  41,\n",
      "         64,  57,  14,  18, 423,  39, 118,  24,  18,  45,  29,   3,  64, 179,\n",
      "         68,   3,   4, 146,  57,  68,  14,  18,  12,  13,  32, 255, 256, 257,\n",
      "         21, 424,  11,  12,  61, 425,  63, 426, 263, 267,  89,  29,  14, 263,\n",
      "         12, 180, 179,  64,  57,  90, 122,  51, 179,  68,   3,  64, 178,  68,\n",
      "          3,  20, 427,  68, 428,  20, 170,  20, 429,  32, 430,  20, 272, 246,\n",
      "        187, 230,  92, 396,  77, 207,  13,  14,  60,  18,  45,  29, 244,  75,\n",
      "        260, 256, 257,   4, 261, 262, 263, 267,  66, 178,  68,   3,  64, 181,\n",
      "        182, 186,   4, 431,  50,  51, 432,  68, 145,   4,  80,  52,  75, 206,\n",
      "         96,  29, 172,  20, 265, 256, 257,   4, 433, 434, 435, 436, 437, 438,\n",
      "        263,  66, 179,  68,   3,   4,  80,  52,  75,  60,  18,  45,  29,   3,\n",
      "         32,  49,  50,   5,  64, 120,  50, 121,  40,  14, 205,  50,   6, 206,\n",
      "         11,  12,  13,   6,  60,  18,  45,  29,  14, 108,   4,   5,  64,  57,\n",
      "         90,  93,  32,  39,  24,  85,  16, 158, 162,  20,  33, 117, 100,  16,\n",
      "         35,  12,  36,  32, 439,  89,  29,  79,  31,  39, 118,   9, 440,   6,\n",
      "          3,   4,  41,  64,  41,  42,  21,  22, 100,   9, 441, 232, 248,  92,\n",
      "         93,  32,  39,  24,  95,  96,  29,   2,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]), 'intention': tensor(5)}\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.shape}')\n",
    "print(x)\n",
    "\n",
    "print(f'y shape: {y[\"answer\"].shape}')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5298"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80%의 데이터를 train에 할당합니다.\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나머지 20% 데이터를 test에 할당합니다.\n",
    "test_size = len(dataset) - train_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 랜덤 스플릿으로 분할을 완료합니다.\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# 배치사이즈 : 16 (16개씩 묶음)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=16, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=16, \n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 401,   68,  145,  357,   81,  110, 1704,  410, 1206,    9,  323,  409,\n",
       "           11,  422,  112,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]),\n",
       " {'answer': tensor([ 401,   68,  145,   12,  401,   21,  998,  145,   75,   20,   30,   31,\n",
       "           357,   81,  467,   12,  647,  510,   64,  336,   50, 1736,   14,   95,\n",
       "            96,   29,  401,   68,  145,    4,  357,   32,  290,    4,  775,   81,\n",
       "          1843,   11,   17,  279,  277,    4, 1674,   81,  140,   11,   12,   13,\n",
       "             6,  483,  106,   29,  172,   20,  285,  286,   55,  853,   81, 1008,\n",
       "            11,   17,  137,  336,  122, 1847,    9,  469,   96,   29,  285,  286,\n",
       "            55,  853,   32,  647,  510,   64,  119,   31,  336,  485,   53,  646,\n",
       "           485,    9,   99,  602,  106,   29,  647,  510,  314,  230,   12,  131,\n",
       "            20, 1868,   20, 2307,  434,  135,    4, 1880,   50,  485,   53,  308,\n",
       "           439,  485,    9, 1465,   96,   29,  485,  314,  230,   12,  145,    4,\n",
       "           239,   68,   14,   18,   12,  137,  336,  122,   81,  469,   11,  232,\n",
       "           248,  336,  485,   20,  704,  705,   50,  485,   20,  308,  439,  485,\n",
       "            24, 1591,   29,  172,   20,   54,  774,  485,    9,   99,   54,    4,\n",
       "           450,   53,   55,   81,  469,   11,  232,  248, 1857,   20, 1856,   20,\n",
       "          1878,  135,   14,  582,  106,   29,   30,   31,  401,   68,  145,  357,\n",
       "            81,  467,   12, 1893,    4,  119,   31,  485,    9,  417,   11,  163,\n",
       "           290,    4,  439,    9, 1008,  589,   96,   29,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0]),\n",
       "  'intention': tensor(8)})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치 데이터를 추출합니다.\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3077,  191,   14,   77,   11,   12,   30,   31,    5,   32,  343,  111,\n",
       "           112,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [   3,    4,  212,  144,  279,  282,  128,   21,  410,  212,   40,   14,\n",
       "            18,  250,  112,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [   3,   81,  357,   11,   12,  268, 1297,   32,  343,  111,  112,    2,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [1070,   64,  287,    4,  279,  282,   75,  211,  145,  212,   81,  489,\n",
       "            11,  232,  515,  337,  102,   32,  410,   13,   14,   18,  250,  112,\n",
       "             2,    0,    0,    0,    0,    0],\n",
       "         [ 191,   81, 2245,  805,  141,  174,   20,  384, 1252,   68, 1338,   81,\n",
       "           576,  141,  139,   18,  250,  112,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [3077,  191,  357,   81,  110, 1704,  410,  553,    4,  152,    9,  649,\n",
       "           589,  839,  112,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [1436,   64,  191,    4,   57,   68,   81,  973,   11,  232,  248,  222,\n",
       "           308,   64,  230,  410,  357,  485,    9,  110,  409,  839,  112,    2,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [3077,  191,   81,  247,   11,  232,  248,  384,  247,  128,   21,  137,\n",
       "           247,  102,   14,   18,  250,  112,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 154,  290,    9,  515,  703,  585,   14,  333,   31,  317,   12,  343,\n",
       "           111,  112,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 401,   68,  145,  357,   81,  110,  232,  248,  410,  647,  648,   81,\n",
       "           649,  589,  839,  112,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 145,  290,   24, 1458,   14,   53,  286,  344,   81,  585,  141,  916,\n",
       "           810,  589,  141,  668,   81,   26,   27,   28,   29,    2,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 145,  101,   81,  515,  801,   50,   51,  102,  144,  169,   75, 1804,\n",
       "            75, 1648,  232,    9,  923,   11,   12,  317,   24,  343,  111,  112,\n",
       "             2,    0,    0,    0,    0,    0],\n",
       "         [2940, 2941,   84,   14,  145,   53,  410,   57,   14,   18,   25,   26,\n",
       "            27,   28,   29,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [ 751,  280,   90, 2708,   64,  191,   32,  410,    5,    6,   77,  141,\n",
       "           139,   18,  422,  112,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [   3,   64,   57,   11,  163,  297,    4,  268,    5,   32,  343,  111,\n",
       "           112,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [   3,    4,    5,   21,   43,   92,  455,  494,  495,   27,  818,  139,\n",
       "            18,  250,  112,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0]]),\n",
       " {'answer': tensor([[191,  32,  93,  ...,   0,   0,   0],\n",
       "          [  3,  32, 539,  ...,   0,   0,   0],\n",
       "          [  3,  32, 145,  ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [191,  32, 308,  ...,   0,   0,   0],\n",
       "          [  3,  32, 185,  ...,   0,   0,   0],\n",
       "          [  3,  32,  54,  ...,   0,   0,   0]]),\n",
       "  'intention': tensor([5, 7, 8, 4, 2, 8, 5, 2, 1, 0, 1, 4, 6, 6, 5, 5])})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 30]), torch.Size([16, 350]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: (batch_size, sequence_length)\n",
    "x.shape, y[\"answer\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 만들기\n",
    " - 기본적으로 seq-to-seq 는 encoder - decoder 구조로 되어있음\n",
    " - 하나의 단어를 임베딩을 통해 벡터로 만들어줌\n",
    " - 단어사전의 있는 모든 단어를 임베딩을 해줌\n",
    " - 배치사이즈가 16개이므로, 질문입력 사이즈는(16x30), 답변 입력사이즈는(16x300)\n",
    " - 여기에 임베딩 차원이 64이므로(단어 하나를 64개의 무언가로 표현) 각각 인코더를 통과하면 (16x30x64),(16x300x64)의 사이즈가 된다.\n",
    " - gru의 히든 레이어 사이즈가 32이므로, 임베딩을 통과한 데이터가 64에서 32로 줄어듬 (16x30x32), (16x300x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 단어 사전의 개수 지정\n",
    "        self.num_vocabs = num_vocabs\n",
    "        # 임베딩 레이어 정의 (number of vocabs, embedding dimension)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        # GRU (embedding dimension)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30])\n",
      "torch.Size([16, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# Embedding Layer의 입/출력 shape에 대한 이해\n",
    "\n",
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "# x의 shape을 변경합니다.\n",
    "# (batch_size, sequence_length) => (sequence_length, batch_size)\n",
    "embedded = embedding(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(embedded.shape)\n",
    "# input:  (sequence_length, batch_size)\n",
    "# output: (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "embedded = embedded.permute(1, 0, 2)\n",
    "print(embedded.shape)\n",
    "# (sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32   \n",
    "\n",
    "gru = nn.GRU(embedding_dim,      # embedding 차원\n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False)\n",
    "\n",
    "# input       : (sequence_length, batch_size, embedding_dim)\n",
    "# h0          : (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "o, h = gru(embedded, None)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabs: 5017\n"
     ]
    }
   ],
   "source": [
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "print(f'number of vocabs: {NUM_VOCABS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 정의\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "# Encoder에 x 통과 후 output, hidden_size 의 shape 확인\n",
    "# input(x)    : (batch_size, sequence_length)\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape)\n",
    "print(h.shape)\n",
    "# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))\n",
    "# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 단어사전 개수\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=False)\n",
    "        \n",
    "        # 최종 출력은 단어사전의 개수\n",
    "        self.fc = nn.Linear(hidden_size, num_vocabs)\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0) # (1, batch_size) 로 변환\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        output = self.fc(output.squeeze(0)) # (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding Layer의 입/출력 shape\n",
    "x = torch.abs(torch.randn(size=(1, 16)).long())\n",
    "print(x)\n",
    "x.shape\n",
    "# batch_size = 16 이라 가정했을 때,\n",
    "# (1, batch_size)\n",
    "# 여기서 batch_size => (1, batch_size) 로 shape 변환을 선행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32])\n",
      "torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    " #GRU Layer의 입/출력 shape에 대한 이해\n",
    "hidden_size = 32\n",
    "\n",
    "gru = nn.GRU(embedding_dim, \n",
    "             hidden_size, \n",
    "             num_layers=1, \n",
    "             bidirectional=False, \n",
    "             batch_first=False, # batch_first=False로 지정\n",
    "            )\n",
    "\n",
    "o, h = gru(embedded)\n",
    "\n",
    "print(o.shape)\n",
    "# output shape: (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state shape: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([16, 5017])\n"
     ]
    }
   ],
   "source": [
    "# 최종 출력층(FC) shape에 대한 이해\n",
    "fc = nn.Linear(32, NUM_VOCABS) # 출력은 단어사전의 개수로 가정\n",
    "\n",
    "output = fc(o[0])\n",
    "\n",
    "print(o[0].shape)\n",
    "print(output.shape)\n",
    "# input : (batch_size, output from GRU)\n",
    "# output: (batch_size, output dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더 -> 디코더 입출력 shape\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                  hidden_size=32, \n",
    "                  embedding_dim=64, \n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 16, 32]) torch.Size([1, 16, 32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "o, h = encoder(x)\n",
    "\n",
    "print(o.shape, h.shape)\n",
    "# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))\n",
    "# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***************\n",
    "x = torch.abs(torch.full(size=(16,), fill_value=SOS_TOKEN, dtype=torch.long))\n",
    "print(x)\n",
    "x.shape\n",
    "\n",
    "# batch_size = 16 이라 가정(16개의 SOS 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64 # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "embedded.shape\n",
    "# embedding 출력\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 5017]), torch.Size([1, 16, 32]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output, decoder_hidden = decoder(x, h)\n",
    "decoder_output.shape, decoder_hidden.shape\n",
    "# (batch_size, num_vocabs), (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        # inputs : (batch_size, sequence_length)\n",
    "        # outputs: (batch_size, sequence_length)\n",
    "        \n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "        \n",
    "        # 리턴할 예측된 outputs를 저장할 임시 변수\n",
    "        # (sequence_length, batch_size, num_vocabs)\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "        \n",
    "        # 인코더에 입력 데이터 주입, encoder_output은 버리고 hidden_state 만 살립니다. \n",
    "        # 여기서 hidden_state가 디코더에 주입할 context vector 입니다.\n",
    "        # (Bidirectional(1) x number of layers(1), batch_size, hidden_size)\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # (batch_size) shape의 SOS TOKEN으로 채워진 디코더 입력 생성********************\n",
    "        decoder_input = torch.full((batch_size,), SOS_TOKEN, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 순회하면서 출력 단어를 생성합니다.\n",
    "        # 0번째는 SOS TOKEN이 위치하므로, 1번째 인덱스부터 순회합니다.\n",
    "        for t in range(0, output_length):\n",
    "            # decoder_input : 디코더 입력 (batch_size) 형태의 SOS TOKEN로 채워진 입력\n",
    "            # decoder_output: (batch_size, num_vocabs)\n",
    "            # decoder_hidden: (Bidirectional(1) x number of layers(1), batch_size, hidden_size), context vector와 동일 shape\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # t번째 단어에 디코더의 output 저장\n",
    "            predicted_outputs[t] = decoder_output\n",
    "            \n",
    "            # teacher forcing 적용 여부 확률로 결정\n",
    "            # teacher forcing 이란: 정답치를 다음 RNN Cell의 입력으로 넣어주는 경우. 수렴속도가 빠를 수 있으나, 불안정할 수 있음\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # top1 단어 토큰 예측\n",
    "            top1 = decoder_output.argmax(1) \n",
    "            \n",
    "            # teacher forcing 인 경우 ground truth 값을, 그렇지 않은 경우, 예측 값을 다음 input으로 지정\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "        \n",
    "        return predicted_outputs.permute(1, 0, 2) # (batch_size, sequence_length, num_vocabs)로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq 입출력 확인\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, \n",
    "                       hidden_size=32, \n",
    "                       embedding_dim=64, \n",
    "                       num_layers=1)\n",
    "# Seq2Seq 정의\n",
    "seq2seq = Seq2Seq(encoder, decoder, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "# print(x.shape, y.shape)\n",
    "# (batch_size, sequence_length), (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 401, 2194,   24,   54,   21,  360,   12,  109,   64,  145,   24,   77,\n",
       "          141,  139,   18,   12,  317,    9,   26,   27,   28,   29,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [   3,   81,  247,   11,   12,  405,   21,  146,  801,   50,   51,  102,\n",
       "           32,  343,  111,  112,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 191,    4,  268,  212,  144,   21, 1436,  780,   14,  415,   16,  422,\n",
       "          112,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 191,   21,   22,  493,   31,  212, 3862,   81,  346,   17,  502,   45,\n",
       "           29,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 401,   68,  145,   21,  238,  301, 2851, 1272,   12,  212,   14, 1650,\n",
       "           25,  346,   17,  502,  503,   29,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 401,   68,  145,   21,   43,  455,   26,   27,   28,   29,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [  54,  222,  122,   51,  145,  357,   81,  110, 1704,  410,  553,    4,\n",
       "         1206,    9,  323,  409,   11,  422,  112,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [2325,  604,   64,  191,  604,   81, 2371,   21,  805,  141,  916,  410,\n",
       "         1338,   14,   77,  141,  139,   18,   25,  346,   17,  502,  503,   29,\n",
       "            2,    0,    0,    0,    0,    0],\n",
       "        [ 185,   68,  145,  101,   81,  248,  410,  553,    4,  337,   14,  801,\n",
       "           50,  316,   26,   27,   28,   29,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 145,    4,  268,    5,   40,  144,  230,  905,  521,   24,  126,  810,\n",
       "          589,  141,   13,   40,   32,  410,   13,   40,   14,   18,  250,  112,\n",
       "            2,    0,    0,    0,    0,    0],\n",
       "        [4661,  582,   14,  145,    4,   80,   68,   64,  138,   14,   18,   25,\n",
       "          346,   17,  502,  503,   29,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 401,   68,  145,  247,    9,  248,  410,  553,    4,  384,   14,  582,\n",
       "           16,   25,   26,   27,   28,   29,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 191,   21,  372,  212,   40,   81,  455,  346,   17,  502,  503,   29,\n",
       "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 145,    9,  860,  232,  248,  410,  337,   81, 2193,  839,  112,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 145,   21,  807,   12,    5,   40,   14,  410,   13,   40,   14,   18,\n",
       "           25,  346,   17,  502,   45,   29,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0],\n",
       "        [ 401,   68,  145,  631,  401,   21,  155,   77,   11,   12,  122,  111,\n",
       "          112,  313,  722,  410,    5,    6,   77,   11,  422,  112,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': tensor([[145,   4,   5,  ...,   0,   0,   0],\n",
       "         [ 61,  62,  53,  ...,   0,   0,   0],\n",
       "         [191,  32, 458,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [337,  32, 145,  ...,   0,   0,   0],\n",
       "         [145,  12, 185,  ...,   0,   0,   0],\n",
       "         [401,  68, 145,  ...,   0,   0,   0]]),\n",
       " 'intention': tensor([5, 9, 7, 8, 7, 6, 8, 2, 4, 5, 5, 9, 7, 4, 5, 6])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "output = seq2seq(x, y['answer'])\n",
    "# print(output.shape)\n",
    "# (batch_size, sequence_length, num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vocabs: 5017\n",
      "======================\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(5017, 256)\n",
      "    (gru): GRU(256, 512)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(5017, 256)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(256, 512)\n",
      "    (fc): Linear(in_features=512, out_features=5017, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDIMG_DIM = 256\n",
    "\n",
    "print(f'num_vocabs: {NUM_VOCABS}\\n======================')\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDIMG_DIM, \n",
    "                  num_layers=1)\n",
    "\n",
    "# Seq2Seq 생성\n",
    "# encoder, decoder를 device 모두 지정\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(5017, 256)\n",
       "  (gru): GRU(256, 512)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 및 모델 튜닝\n",
    " - 학습시키면서 성능 향상을 위해 하이퍼 파라미터 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련에 적용할 하이퍼파라미터 설정\n",
    "\n",
    "LR = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "es = EarlyStopping(patience=5, \n",
    "                   delta=0.001, \n",
    "                   mode='min', \n",
    "                   verbose=True\n",
    "                  )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=2,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련에 적용할 하이퍼파라미터 설정\n",
    "\n",
    "LR = 5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "es = EarlyStopping(patience=7, \n",
    "                   delta=0.01, \n",
    "                   mode='min', \n",
    "                   verbose=True\n",
    "                  )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                 mode='min', \n",
    "                                                 factor=0.5, \n",
    "                                                 patience=3,\n",
    "                                                 threshold_mode='abs',\n",
    "                                                 min_lr=1e-8, \n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수 정의\n",
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y['answer'].to(device) # tensor로만 학습이 되기때문에, 딕셔너리안에 있는 답변에 해당하는 값을 가져와야함\n",
    "        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # 초기화\n",
    "        \n",
    "        # output: (batch_size, sequence_length, num_vocabs)\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "        \n",
    "        # 1번 index 부터 슬라이싱한 이유는 0번 index가 SOS TOKEN 이기 때문\n",
    "        # (batch_size*sequence_length, num_vocabs) 로 변경\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        \n",
    "        # (batch_size*sequence_length) 로 변경\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation 함수 정의\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y['answer'].to(device)\n",
    "            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)\n",
    "            output = model(x, y)\n",
    "            output_dim = output.size(2)\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            eval_loss += loss.item() * x.size(0)\n",
    "            \n",
    "    return eval_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 샘플링 후 결과 추론\n",
    "def sequence_to_sentence(sequences, index2word):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "\n",
    "        word = index2word[p]\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence를 다시 문장으로 바꾸어 문장 형식으로 출력하기 위한 함수\n",
    "\n",
    "def random_evaluation(model, dataset, index2word, device, n=10):\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)      # Shuffle\n",
    "    sampled_indices = indices[:n]   # Sampling N indices\n",
    "    \n",
    "    # 샘플링한 데이터를 기반으로 DataLoader 생성\n",
    "    sampler = SubsetRandomSampler(sampled_indices)\n",
    "    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in sampled_dataloader:\n",
    "            x, y = x.to(device), y['answer'].to(device)      \n",
    "            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  \n",
    "            output = model(x, y, teacher_forcing_ratio=0)\n",
    "            # output: (number of samples, sequence_length, num_vocabs)\n",
    "            \n",
    "            preds = output.detach().cpu().numpy()\n",
    "            x = x.detach().cpu().numpy()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(n):\n",
    "                print(f'질문   : {sequence_to_sentence(x[i], index2word)}')\n",
    "                print(f'답변   : {sequence_to_sentence(y[i], index2word)}')\n",
    "                print(f'예측답변: {sequence_to_sentence(preds[i].argmax(1), index2word)}')\n",
    "                print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 입력받아 답을 출력하는 함수\n",
    "\n",
    "def predict(model, sentence, index2word, device, n=10):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tokens = dataset.texts_to_sequences(dataset.clean_text(sentence))\n",
    "        input_padded = dataset.pad_sequence(input_tokens, dataset.q_max_length)\n",
    "    \n",
    "        # 입력 데이터를 텐서로 변환\n",
    "        input_tensor = torch.tensor(input_padded).unsqueeze(0).to(device)  # 배치 차원을 추가하고 텐서로 변환\n",
    "        output_tensor =  torch.tensor([0 for i in range(350)]).unsqueeze(0).to(device)\n",
    "        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  \n",
    "        output = model(input_tensor, output_tensor, teacher_forcing_ratio=0)\n",
    "        # output: (number of samples, sequence_length, num_vocabs)\n",
    "        \n",
    "        # preds = output.detach().cpu().numpy()\n",
    "        # x = x.detach().cpu().numpy()\n",
    "        # y = y.detach().cpu().numpy()\n",
    "        \n",
    "        output_tokens = output.detach().squeeze(0).cpu().numpy()  # 배치 차원을 제거하고 넘파이 배열로 변환\n",
    "        predicted_tokens = np.argmax(output_tokens, axis=1)  # 각 시퀀스의 최대 확률 토큰을 선택\n",
    "        \n",
    "        response_sentence = sequence_to_sentence(predicted_tokens, index2word)  # 토큰 시퀀스를 문자열로 변환\n",
    "        \n",
    "        return response_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  훈련 시작\n",
    "# NUM_EPOCHS = 5\n",
    "# STATEDICT_PATH = 'seq2seq-chatbot-kor.pt'\n",
    "\n",
    "# best_loss = np.inf\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    \n",
    "#     val_loss = evaluate(model, test_loader, loss_fn, device)\n",
    "     \n",
    "#     if val_loss < best_loss:\n",
    "#         best_loss = val_loss\n",
    "#         torch.save(model.state_dict(), STATEDICT_PATH)\n",
    "    \n",
    "#     if epoch % 5 == 0:\n",
    "#         print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "    \n",
    "#     # Early Stop\n",
    "#     es(loss)\n",
    "#     if es.early_stop:\n",
    "#         break\n",
    "    \n",
    "#     # Scheduler\n",
    "#     scheduler.step(val_loss)\n",
    "                   \n",
    "# model.load_state_dict(torch.load(STATEDICT_PATH))\n",
    "# torch.save(model.state_dict(), f'seq2seq-chatbot-kor-{best_loss:.4f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dict.pkl', 'rb') as f:\n",
    "    dict1=pickle.load(f)\n",
    "    dict2 = {v : k for k, v in dict1.items()}\n",
    "dataset.wordvocab.word2index = dict1\n",
    "dataset.wordvocab.index2word = dict2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문   : 치매 예방 을 위해 어떤 운동 이 좋 을까요 ?\n",
      "답변   : 치매 환자 를 위한 운동 은 다양 한 종류 가 있 습니다 . 근력 강화 를 위해 근력 훈련 , 근육 운동 , 유산소 운동 , 스트레칭 과 같 은 유산소 운동 과 유연 성 을 향상 시키 는 운동 이 유용 합니다 . 또한 , 관절 을 강화 하 는 요가 와 수중 운동 , 태극 권 과 같이 수중 운동 도 추천 됩니다 . 인지 훈련 을 위해서 는 미술 , 음악 , 게임 과 같 은 인지 활동 이 좋 습니다 . 마지막 으로 , 유산소 운동 과 병행 하 여 근력 운동 을 함께 하 는 것 이 치매 발병 위험 을 줄이 는 데 도움 이 될 수 있 습니다 . 치매 환자 들 은 운동 을 통해 인지 기능 을 유지 하 고 일상 생활 에 더 활력 을 얻 을 수 있 습니다 . 이 를 위해서 는 전문가 와 상담 하 여 적절 한 운동 프로그램 을 계획 하 고 진행 하 는 것 이 중요 합니다 .\n",
      "예측답변: 치매 환자 들 에게 는 운동 은 다양 합니다 . 치매 환자 들 에게 는 운동 운동 은 신체 활동 을 유지 하 고 인지 기능 을 향상 시키 는 데 도움 이 됩니다 . 운동 은 치매 환자 의 신체 적 인 활동 을 유지 하 고 인지 기능 을 향상 시키 는 데 도움 이 됩니다 . 유산소 운동 은 근력 운동 은 근육 을 강화 하 고 신체 기능 을 향상 시키 는 데 도움 이 됩니다 . 또한 , 운동 은 근육 을 강화 하 고 치매 환자 의 을 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 의 운동 은 신체 활동 과 인지 능력 을 향상 시키 고 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 의 운동 은 신체 적 인 운동 을 통해 하 고 , 신체 활동 을 유지 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 치매 예방 을 위해 어떤 운동 을 추천 하 시 는지 알려 주 세요 .\n",
      "답변   : 운동 은 치매 환자 들 에게 필수 적 인 활동 입니다 . 치 매 는 일상 생활 을 수행 하 는 능력 을 저하 시키 는 원인 으로 , 신체 활동 이 치매 발병 을 예방 하 고 치료 하 는 데 중요 한 역할 을 합니다 . 알츠하이머병 은 치매 환자 의 뇌 에 혈액 공급 이 감소 하 여 증상 이 발생 하 는 질병 이 므로 , 정기 적 인 신체 활동 은 필수 적 입니다 . 걷 기 , 수영 , 자전거 타기 등 의 유산소 운동 은 뇌 로 혈류 를 증가 시켜 치매 증상 을 완화 시키 는 데 도움 을 줍니다 . 또한 근력 운동 과 균형 운동 은 근육 의 기능 을 향상 시키 고 신체 활동 량 을 증가 시켜 치매 증상 을 예방 하 고 개선 하 는 데 효과 적 입니다 . 운동 은 치매 환자 들 에게 필수 적 인 활동 으로 , 신체 활동 량 을 꾸준히 유지 하 여 치매 증상 의 개선 과 예방 에 도움 을 줄 수 있 습니다 .\n",
      "예측답변: 치매 환자 의 운동 은 다양 한 이점 을 제공 할 수 있 습니다 . 치매 환자 들 은 신체 적 인 활동 을 통해 하 고 인지 기능 을 유지 하 는 데 도움 이 됩니다 . 운동 은 치매 환자 의 신체 활동 을 을 향상 시키 고 인지 기능 을 향상 시키 는 데 도움 이 됩니다 . 유산소 운동 은 근력 운동 은 근육 을 강화 하 고 뇌 로 혈액 공급 을 원활 하 게 공급 하 는 데 도움 이 됩니다 . 또한 , 운동 은 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 의 운동 은 신체 활동 과 인지 능력 을 향상 시키 고 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 의 운동 은 신체 적 인 운동 을 통해 하 고 , 신체 활동 을 유지 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 약물 복용 중 인데 , 약물 을 임의 로 중단 하 는 것 이 좋 은지 알 고 싶 습니다 .\n",
      "답변   : 우울증 은 다양 한 치료 접근 법 으로 관리 될 수 있 습니다 . 약물 치료 외 에 도 스트레스 관리 , 정서 적 지원 , 인지 행동 치료 등 을 통해 우울증 치료 에 도움 을 줄 수 있 습니다 . 스트레스 관리 를 통해 우울증 을 유발 하 는 스트레스 요인 을 파악 하 고 대응 하 는 것 이 중요 합니다 . 또한 , 인지 행동 치료 는 우울증 의 심리 적 인 문제 를 극복 하 기 위해 필요 한 것 으로 , 올바른 사고 패턴 과 행동 패턴 을 학습 하 여 인지 재 구조 화 를 돕 습니다 . 마지막 으로 , 우울증 치료 에 는 환자 와 의 심리 적 인 상담 도 필요 합니다 . 위 의 다양 한 치료 접근 법 을 통해 우울증 을 효과 적 으로 관리 할 수 있 습니다 .\n",
      "예측답변: 우울증 치료 에 사용 되 는 약물 중 하나 는 다양 한 종류 가 있 습니다 . ssri 는 우울증 의 증상 을 완화 하 고 데 도움 을 주 는 , 입니다 . 이 약물 은 뇌 내 의 신경전달물질 의 균형 을 조절 하 여 우울증 증상 을 완화 시킵니다 . 또한 , 항우울제 는 우울증 증상 을 완화 하 고 데 도움 을 줄 수 있 습니다 . 그러나 약물 치료 는 에 도 인지 행동 치료 , 인지 치료 , 인지 치료 등 다양 한 치료 방법 이 있 습니다 . 약물 치료 는 우울증 의 증상 을 완화 하 고 삶 의 질 을 향상 시키 는 데 도움 을 줄 수 있 습니다 . 약물 치료 는 우울증 의 치료 에 사용 되 는 약물 중 하나 입니다 . 그러나 약물 치료 는 개인 의 상황 과 증상 에 맞 는 치료 계획 을 수립 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 치매 로 인한 기억력 손상 을 관리 하 기 위한 식 이 와 생활 습관 은 어떤 것 들 이 있 을까요 ?\n",
      "답변   : 담배 는 흡연 은 혈관 을 좁 게 만들 고 뇌세포 에 산소 공급 을 저해 하 며 , 이 로 인해 인지 기능 저하 및 치매 발생 의 위험 이 있 다고 알려져 있 습니다 . 연구 에 따르 면 담배 를 피우 는 사람 들 이 비흡연자 에 비해 혈관 성 치매 발생 위험 이 1 . 79 배 , 혈관 성 치매 의 위험 성 이 1 . 78 배 높 습니다 . 또한 , 흡연 으로 인해 인지 기능 의 저하 가 진행 될 수 있 으며 , 뇌세포 의 혈액 순환 도 제한 되 어 알츠하이머병 의 위험 도 증가 할 수 있 습니다 . 따라서 담배 를 피우 는 것 은 건강 에 좋 지 않 으며 , 특히 흡연 중 인 분 들 은 금연 을 권장 합니다 .\n",
      "예측답변: 담배 는 치매 의 위험 을 증가 시키 는 있 습니다 . 담배 는 뇌세포 에 유해 한 화학 물질 은 뇌세포 의 손상 을 방해 하 고 , 이 로 인해 뇌세포 의 손상 을 초래 합니다 . 이 로 인해 뇌세포 가 손상 되 고 , 이 이 저하 되 고 , 이 로 인해 인지 기능 저하 가 발생 합니다 . 이러 한 이유 로 알츠하이머병 은 혈관 성 치매 로 인해 발생 하 는 치매 의 위험 이 증가 시킵니다 . 또한 , 흡연 은 혈관 성 치매 의 위험 을 증가 시키 는 주요 원인 중 하나 입니다 . 담배 는 흡연 은 혈관 을 좁 아 치매 발생 위험 을 증가 시킵니다 . 또한 , 흡연 은 알츠하이머병 의 위험 을 증가 시키 는 데 에 도 효과 적 입니다 . 담배 를 피우 는 것 은 치매 의 위험 을 증가 시키 고 , 인지 기능 저하 를 위험 을 증가 시킵니다 .\n",
      "==============================\n",
      "질문   : 치매 예방 을 위한 운동 방법 이 있 을까요 ? 어떤 운동 이 좋 을까요 ?\n",
      "답변   : 치매 는 예방 가능 한 질병 입니다 . 치매 예방 을 위해 생활 습관 과 신체 적 활동 을 유지 하 는 것 이 중요 합니다 . 운동 이나 활발 한 사회 적 활동 은 뇌 를 활발 하 게 하 고 혈액 순환 이 촉진 되 며 스트레스 를 해소 할 수 있 습니다 . 또한 , 올바른 식습관 과 충분 한 휴식 을 취함 으로써 뇌 기능 이 활성 화 됩니다 . 치매 예방 을 위해서 는 이러 한 생활 습관 과 신체 적 활동 을 실천 해야 합니다 . 이러 한 노력 은 우리 의 건강 과 행복 을 증진 시키 는 데 도움 이 될 것 입니다 .\n",
      "예측답변: 치매 예방 을 위해 는 한 운동 은 뇌 건강 을 유지 하 는 것 이 중요 합니다 . 치매 환자 들 은 신체 적 인 운동 을 통해 신체 활동 을 유지 하 고 인지 기능 을 향상 시킬 수 있 습니다 . 운동 은 치매 환자 의 신체 활동 을 을 향상 시키 고 인지 기능 을 향상 시키 는 데 도움 이 됩니다 . 또한 , 운동 은 을 통해 하 고 신체 활동 을 유지 하 는 것 이 중요 합니다 . 운동 은 치매 환자 의 신체 적 인 건강 을 유지 하 고 치매 의 위험 을 줄일 수 있 습니다 . 또한 , 운동 은 신체 활동 은 치매 환자 의 일상 생활 능력 을 향상 시키 고 일상 생활 능력 을 향상 시키 는 데 도움 이 됩니다 . 치매 환자 의 운동 은 신체 적 인 운동 을 통해 하 고 , 신체 활동 을 유지 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 알츠하이머 진단 을 받 기 위해 필요 한 검사 나 절차 가 무엇 인가요 ?\n",
      "답변   : 치매 진단 은 복잡 한 절차 로 이루어지 며 , 증상 의 정확 한 평가 와 검사 가 필요 합니다 . 치 매 는 연령 과 함께 진행 되 며 , 초기 에 는 증상 이 미미 하 기 때문 에 환자 와 가족 이 치매 진단 을 받 기 까지 는 시간 이 소요 됩니다 . 초기 단계 에서 는 행동 및 인지 기능 의 손상 이 명확 하 지 않 기 때문 에 , 이 를 통해 정확 한 판단 을 내릴 수 없 습니다 . 그러나 최근 에 는 현대 의학 의 발전 으로 인해 mri 영상 분석 이 가능 해 지 면서 초기 치 매 진단 이 용 이 하 게 되 었 습니다 . mri 영상 분석 은 초기 치매 를 진단 하 는 데 매우 유용 한 도구 이 며 , 정확 한 진단 을 통해 치매 환자 의 조기 치료 와 관리 를 돕 는데 도움 을 줄 수 있 습니다 .\n",
      "예측답변: 치매 는 노화 로 인한 뇌 질환 으로 , 초기 치매 의 진단 하 는 데 에 진단 하 는 것 이 중요 합니다 . 치매 의 진단 은 는 다양 한 검사 가 사용 됩니다 . 먼저 , 치매 의 진단 을 위해 는 한 검사 가 사용 됩니다 . 먼저 , 치매 의 진단 을 위해 는 한 검사 가 필요 합니다 . 이 를 통해 환자 의 인지 능력 을 평가 하 고 , 신체 검사 , 신 경학 적 검사 , 정신 상태 검사 , 혈액 검사 , 뇌 영상 검사 , 핵의학 검사 , 뇌척수액 검사 등 을 통해 치매 의 원인 을 찾 을 수 있 습니다 . 이러 한 검사 를 통해 치매 의 원인 을 정확히 파악 하 고 , 한 치료 계획 을 수립 할 수 있 습니다 .\n",
      "==============================\n",
      "질문   : 알코올 성 치매 를 의심 할 때 , 어떤 절차 를 거쳐야 할까요 ?\n",
      "답변   : 알코올 성 치매 는 알코올 의 과다 한 소비 로 인해 발생 하 는 신경 손상 과 인지 능력 의 저하 로 특징지 어 집니다 . 알코올 성 치매 의 초기 진단 은 일반 적 으로 간단 한 신체 검사 와 인지 기능 평가 로 이루어집니다 . 신체 검사 에서 는 혈액 검사 와 뇌 영상 검사 를 통해 알코올 성 치매 와 관련 된 뇌 의 이상 소견 을 확인 할 수 있 습니다 . 알코올 성 치매 는 인지 기능 이 저하 되 어 일상 생활 과 사회 적 기능 에 영향 을 미치 는데 , 이 를 확인 하 기 위해 인지 기능 검사 가 수행 됩니다 . 또한 음주 에 따른 뇌 내 화학 물질 변화 와 뇌 부위 의 혈액 흐름 여부 를 확인 하 여 알코올 성 치매 를 평가 합니다 . 알코올 성 치매 는 뇌 의 손상 과 인지 기능 저하 를 초래 할 수 있 는 위험 한 질병 입니다 . 정확 한 진단 과 조기 치료 를 통해 이 질병 의 진행 을 막 고 사회 적 , 기능 적 기능 을 보존 할 수 있 습니다 .\n",
      "예측답변: 알코올 성 치매 의 진단 은 다양 한 방법 을 통해 이루어집니다 . 알코올 성 치매 의 진단 은 병력 청취 와 신체 검사 점검 을 통해 이루어집니다 . 의사 는 환자 의 병력 을 듣 고 , 신체 검사 , 신 경학 적 검사 , 정신 상태 검사 를 통해 합니다 . 이후 에 는 신체 검사 , 신 경학 적 검사 , 정신 상태 검사 를 실시 합니다 . 신체 검사 , 신 경학 적 검사 , 정신 상태 검사 를 통해 인지 기능 의 변화 를 확인 합니다 . 또한 , 혈액 검사 , 소변 검사 , 흉부 x 선 검사 , 심전 도 검사 등 을 통해 다른 신체 질환 을 확인 합니다 . 뇌 영상 검사 를 통해 뇌 의 구조 와 기능 을 확인 하 고 , 뇌 영상 검사 를 통해 뇌 의 구조 와 기능 을 확인 합니다 . 알코올 성 치매 의 진단 은 병력 청취 와 신체 검사 , 신 경학 적 검사 , 정신 상태 검사 , 그리고 검사 , 흉부 x 선 검사 , 심전 도 검사 등 을 통해 이루어집니다 .\n",
      "==============================\n",
      "질문   : 알츠하이머 를 예방 하 기 위해 권장 되 는 운동 은 어떤 것 이 있 을까요 ?\n",
      "답변   : 치매 는 신경 인지 기능 과 운동 기능 의 장애 를 동반 하 는 질병 입니다 . 주로 우울증 , 불안증 , 인지 기능 저하 , 알츠하이머병 과 관련 이 있 습니다 . 이러 한 질환 을 관리 하 기 위해서 는 뇌 자극 을 위한 운동 이 필요 합니다 . 치매 환자 의 운동 은 기억력 , 학습 능력 , 집중력 , 운동 수행 능력 등 을 향상 시키 는 데 도움 을 줄 수 있 습니다 . 운동 은 또한 사회 적 인 능력 과 감정 조절 에 도 도움 을 줄 수 있 습니다 . 치매 증상 을 완화 하 기 위해서 는 규칙 적 인 운동 을 생활 화 해야 합니다 . 또한 신체 활동 을 통해 인지 기능 의 저하 를 지연 시키 고 치매 환자 의 삶 의 질 을 향상 시킬 수 있 습니다 . 치매 환자 에게 적절 한 운동 은 신체 와 인지 기능 을 향상 시키 는 데 도움 이 되 는 중요 한 요소 입니다 . 꾸준 한 신체 활동 을 통해 치매 환자 의 삶 의 질 을 개선 하 고 질병 을 관리 할 수 있 습니다 .\n",
      "예측답변: 치매 는 인지 기능 의 저하 로 인해 일상 생활 에 어려움 을 겪 는 질환 입니다 . 치매 예방 을 위해 는 한 운동 을 실천 하 는 것 이 중요 합니다 . 치매 예방 을 위해서 는 규칙 적 인 운동 과 건강 한 식습관 , 스트레스 관리 , 건강 한 식습관 , 스트레스 관리 등 을 통해 치매 의 위험 을 줄일 수 있 습니다 . 또한 , 건강 한 식습관 을 유지 하 고 , 스트레스 관리 를 통해 치매 예방 에 도움 이 됩니다 . 또한 , 건강 한 식습관 과 충분 한 수면 을 취하 고 , 스트레스 관리 기술 을 익히 는 것 도 중요 합니다 . 치매 예방 을 위해서 는 건강 한 라이프 스타일 을 유지 하 고 , 꾸준 한 운동 을 통해 신체 활동 을 유지 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 알츠하이머 예방 을 위해 어떤 예방 접종 이 권 장 되 는지 알려 주 세요 .\n",
      "답변   : 치매 를 예방 하 기 위해서 는 건강 한 생활 습관 을 유지 하 는 것 이 중요 합니다 . 치매 예방 을 위한 특별 한 방법 은 없 지만 , 일상 생활 에서 조금 만 신경 을 쓰 면 치매 발병 위험 을 줄일 수 있 습니다 . 규칙 적 인 운동 과 올바른 식습관 , 스트레스 관리 는 치매 예방 에 큰 도움 이 됩니다 . 규칙 적 인 운동 을 통해 뇌 에 물리 적 인 자극 을 주 고 , 스트레스 를 관리 하 면 치매 예방 에 도움 이 됩니다 . 또한 , 두뇌 활동 에 는 퍼즐 , 독서 , 게임 등 의 활동 을 포함 하 는 것 이 좋 습니다 . 이 외 에 도 사회 적 인 활동 과 독서 , 퍼즐 , 보드게임 등 의 활동 을 통해 두뇌 를 활발 하 게 사용 하 고 다양 한 자극 을 주 는 것 이 중요 합니다 . 치매 예방 을 위해서 는 건강 한 생활 습관 을 유지 하 고 , 규칙 적 인 운동 과 올바른 식습관 을 유지 하 는 것 이 중요 합니다 . 또한 , 일상 생활 에서 뇌 를 자극 할 수 있 는 활동 과 독서 , 퍼즐 , 보드게임 등 을 경험 하 는 것 도 도움 이 됩니다 .\n",
      "예측답변: 치매 예방 을 위해 는 한 생활 습관 을 유지 하 는 것 이 중요 합니다 . 규칙 적 인 운동 은 뇌 건강 에 도움 이 되 는 활동 을 뇌 건강 을 유지 하 는 데 도움 이 됩니다 . 또한 , 건강 한 식습관 을 유지 하 는 것 도 중요 합니다 . 규칙 적 인 운동 은 뇌 건강 을 유지 하 고 치매 발병 위험 을 줄일 수 있 습니다 . 또한 , 건강 한 식습관 을 유지 하 고 , 스트레스 를 관리 하 는 것 도 치매 예방 에 도움 이 됩니다 . 또한 , 건강 한 식단 을 유지 하 고 , 한 한 을 유지 하 는 것 도 치매 예방 에 도움 이 됩니다 . 치매 예방 을 위해서 는 건강 한 생활 습관 을 유지 하 고 , 적절 한 운동 을 통해 하 는 것 이 중요 합니다 .\n",
      "==============================\n",
      "질문   : 알츠하이머병 의 증상 중 가까운 기억 들 의 기억력 이 저하 되 는 것 이 있 는지 알 고 싶 어요 .\n",
      "답변   : 알츠하이머병 은 신경 세포 가 소실 되 는 퇴행 성 신경 질환 으로 , 주로 노인 들 에게서 발생 합니다 . 이 질환 은 점진 적 으로 진행 되 어 기억력 , 판단력 , 일상 생활 능력 등 다양 한 인지 기능 에 영향 을 줍니다 . 알츠하이머병 의 초기 증상 은 기억 손상 , 언어 장애 , 시간 과 장소 혼동 , 행동 및 정신 적 증상 입니다 . 또한 알츠하이머병 은 성격 변화 , 우울증 , 망상 , 환각 , 언어 장애 , 행동 이상 등 의 심리 적 증상 과 병 으로 인한 신체 적 장애 도 나타낼 수 있 습니다 . 알츠하이머병 의 치료 에 는 주로 병 의 진행 을 늦추 고 환자 의 생활 의 질 을 향상 시키 는 것 을 목표 로 합니다 . 약물 치료 는 알츠하이머병 을 치료 하 는 중요 한 방법 중 하나 로 , 정신 적 인 증상 의 완화 와 삶 의 질 향상 을 위한 목적 으로 사용 됩니다 . 조기 진단 과 개인 별 맞춤 치료 계획 은 치매 관리 와 예방 에 있 어서 가장 효과 적 입니다 .\n",
      "예측답변: 알츠하이머병 은 치매 의 가장 흔한 형태 로 , 주로 노인 들 에게 발생 하 는 치매 의 한 형태 입니다 . 알츠하이머병 은 아밀로이드 베타 단백질 과 타우 단백질 의 비 정상 적 인 축 적 으로 인해 발생 합니다 . 알츠하이머병 은 뇌 의 신경 세포 의 손상 으로 인해 발생 하 며 , 이 는 점진 적 으로 악화 되 는 것 이 특징 입니다 . 알츠하이머병 은 초기 증상 은 기억력 저하 와 언어 능력 저하 , 판단력 저하 , 의 증상 을 보입니다 . 알츠하이머병 은 진행 되 면 일상 생활 에 큰 영향 을 미치 며 , 조기 에 과 적절 한 치료 가 필요 합니다 . 알츠하이머병 은 치료 가 불 가능 하 지만 , 조기 진단 과 적절 한 치료 가 필요 합니다 .\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "STATEDICT_PATH = 'seq2seq-chatbot-kor22.pt'\n",
    "model.load_state_dict(torch.load(STATEDICT_PATH, map_location=device))\n",
    "random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "def response(message, history, additional_input_info):\n",
    "    # additional_input_info의 텍스트를 챗봇의 대답 뒤에 추가합니다.\n",
    "    response = predict(model, message, dataset.wordvocab.index2word, device)\n",
    "    return response\n",
    "gr.ChatInterface(\n",
    "        fn=response,\n",
    "        textbox=gr.Textbox(placeholder=\"무엇이든 물어보세요\", container=False, scale=7),\n",
    "        title=\"치매박사🌞안깜빡이에요~\",\n",
    "        description=\"뇌신경질환(치매,알코올성치매,알츠하이머병,우을증)에 대해 무엇이든 물어보세요.\",\n",
    "        theme=\"soft\",\n",
    "        examples=[[\"치매가 의심될때는 어떤 전문의를 방문해야 하나요?\"], [\"알코올성 치매를 예방하는 가장 효과적인 방법은요?\"], [\"우울증 약물의 부작용은요?\"]],\n",
    "        retry_btn=\"다시보내기 ✈\",\n",
    "        undo_btn=\"이전챗 삭제 ✂\",\n",
    "        clear_btn=\"전챗 삭제 💥\",\n",
    "        additional_inputs=[\n",
    "            gr.Textbox(\"!!!\", label=\"끝말잇기\")\n",
    "        ]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
