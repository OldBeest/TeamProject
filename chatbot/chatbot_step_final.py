#!/usr/bin/env python
# coding: utf-8

# ## ê°€ìƒí™˜ê²½ì„¤ì •
#     ì•„ë‚˜ì½˜ë‹¤ ê°€ìƒí™˜ê²½ ì„¤ì •í•˜ê¸°
#   - conda create -n med_chatbot python=3.9
#   - conda activate  med_chatbot
#   - pip install torch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 (cpuë§Œ ì‚¬ìš©í• ë•Œ)
#   - pip install python-mecab-ko
#   - pip install sentence-transformers
#   - pip install pandas
#   - pip install matplotlib
#   - pip install numpy==1.26.4 (ì œì¼ ì¤‘ìš”!(numpyê°€ 2.0.1ë¡œ ê¹”ë ¤ìˆì§€ë§Œ, ì›í™œí•œ í•¨ìˆ˜ ì‚¬ìš©ì„ ìœ„í•´ 1.26.4ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ í•´ì¤˜ì•¼í•¨))

# In[6]:


import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import numpy as np
import pandas as pd
from sentence_transformers import util
from mecab import MeCab

import json
from glob import glob


# ## json ë°ì´í„° ë¶ˆëŸ¬ì™€ ë°ì´í„° í”„ë ˆì„ ë§Œë“¤ê¸°
#  1. globìœ¼ë¡œ í•´ë‹¹ í´ë” ëª¨ë“  json íŒŒì¼ê²½ë¡œë¥¼ listë¡œ ë¶ˆëŸ¬ì˜´
#  2. json.loadë¥¼ ì´ìš©í•´ ì§ˆë¬¸, ë‹µë³€, ì˜ë„ë¥¼ ê°ê° ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ë‹´ëŠ”ë‹¤
#  3. ì§ˆë¬¸, ë‹µë³€, ì˜ë„ì— ëŒ€í•œ ë°ì´í„° í”„ë ˆì„ì„ ë§Œë“¤ê³  concatìœ¼ë¡œ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹œë‹¤

# In[7]:


def all_data(path1, path2):
    question_path = path1
    answer_path = path2

    return glob(question_path + '/*'), glob(answer_path + '/*')


# In[8]:


disease_q_path, disease_a_path = all_data('./training/ë¼ë²¨ë§ë°ì´í„°/ì§ˆë¬¸/' , './training/ë¼ë²¨ë§ë°ì´í„°/ë‹µë³€/') # ê²½ë¡œ ì…ë ¥


# In[9]:


disease_q_path # q_data : ì˜ë„ë³„ í´ë”


# In[10]:


q_list = []
a_list = []
i_list = []
for intention_q_path, intention_a_path in zip(disease_q_path, disease_a_path):
    intention_q_folders, intention_a_folders = glob(intention_q_path + '/*'), glob(intention_a_path + '/*')
    
    for q_intention, a_intention  in zip(intention_q_folders, intention_a_folders):
        q_json_files = glob(q_intention + '/*')
        a_json_files = glob(a_intention + '/*')
        
        # print(q_intention, len(q_json_files), a_intention, len(a_json_files))
        # print(min([len(q_json_files), len(a_json_files)]))
        
        for idx in range(min([len(q_json_files), len(a_json_files)])):
            with open(q_json_files[idx], 'r', encoding='utf-8') as file:
                q_json_data = json.load(file)
                q_list.append(q_json_data['question'])
                i_list.append(q_json_data['intention'])
            with open(a_json_files[idx], 'r', encoding='utf-8') as file:
                a_json_data = json.load(file)
                sentence = ""
                for key in a_json_data['answer'].keys():
                    sentence += a_json_data['answer'][key]
                a_list.append(sentence)


# In[11]:


q_df = pd.DataFrame(q_list)
i_df = pd.DataFrame(i_list)
a_df = pd.DataFrame(a_list)


# In[12]:


df = pd.concat((q_df, i_df, a_df), axis=1)
df.columns = ['question', 'intention', 'answer']


# In[13]:


df


# In[14]:


df.to_csv("dementia_fix.csv", sep=',', encoding='utf-8')


# In[15]:


# df = df.sample(frac=1).reset_index(drop=True)
# df.to_csv("dementia_shuffle.csv", sep=',', encoding='utf-8')


# In[16]:


# def all_data(path1, path2):
#     question_path = path1
#     answer_path = path2

#     return glob(question_path + '/*/*.json'), glob(answer_path + '/*/*.json')


# In[17]:


# q_data, a_data = all_data('./dementia/training/ì›ì²œë°ì´í„°/ì§ˆë¬¸/ì¹˜ë§¤', './dementia/training/ì›ì²œë°ì´í„°/ë‹µë³€/ì¹˜ë§¤') # ê²½ë¡œ ì…ë ¥


# In[18]:


# # ì¹˜ë§¤ ë°ì´í„° ê°œìˆ˜ í™•ì¸ (ì§ˆë¬¸, ë‹µë³€)
# len(q_data), len(a_data)


# In[19]:


# with open(q_data[1000], 'r', encoding='utf-8') as file:
#     json_data = json.load(file)
#     print(json_data)


# In[20]:


# q_list = []
# for i in range(len(q_data)):
#     with open(q_data[i], 'r', encoding='utf-8') as file:
#         json_data = json.load(file)
#         q_list.append(json_data['question'])


# In[21]:


# a_list = []
# for i in range(len(q_data)):
#     with open(a_data[i], 'r', encoding='utf-8') as file:
#         json_data = json.load(file)
#         sentence = ""
#         for key in json_data['answer']:
#             sentence += json_data['answer'][key]
#         a_list.append(sentence)


# In[22]:


# i_list=[]
# for i in range(len(q_data)):
#     with open(q_data[i],'r',encoding='utf-8') as file:
#         json_data=json.load(file)
#         sentence = ""
#         sentence += json_data['intention']
#         i_list.append(sentence)


# In[23]:


# df = pd.read_csv('dementia_shuffle.csv', encoding='utf-8', index_col=0)


# In[24]:


# # ë°ì´í„° í”„ë ˆì„ ë§Œë“¤ê¸°
# q_df = pd.DataFrame(q_list) # ì§ˆë¬¸ ë°ì´í„° í”„ë ˆì„
# a_df = pd.DataFrame(a_list) # ë‹µë³€ ë°ì´í„° í”„ë ˆì„
# i_df = pd.DataFrame(i_list) # ì˜ë„ ë°ì´í„° í”„ë ˆì„
# qa_df = pd.concat((q_df, a_df), axis=1) # ì§ˆë¬¸-ì˜ë„ ë°ì´í„° í”„ë ˆì„
# qa_df.columns=['question', 'answer']
# qi_df = pd.concat((q_df, i_df), axis=1) # ì§ˆë¬¸-ë‹µë³€ ë°ì´í„° í”„ë ˆì„
# qi_df.columns=['question', 'intention']
# df = pd.concat((q_df, i_df, a_df), axis=1) # ì§ˆë¬¸-ì˜ë„-ë‹µë³€ ë°ì´í„° í”„ë ˆì„
# df.columns=['question', 'intention', 'answer']


# In[25]:


# # csv íŒŒì¼ë¡œ ì €ì¥
# df.to_csv("dementia_qia.csv", sep=',') #ì§ˆë¬¸, ì˜ë„, ë‹µë³€ í¬í•¨
# qa_df.to_csv("dementia_qa.csv", sep=',') #ì§ˆë¬¸, ë‹µë³€
# qi_df.to_csv("dementia_qi.csv", sep=',') #ì§ˆë¬¸, ì˜ë„ -> ë‚˜ì¤‘ì— ì˜ë„ ë¶„ë¥˜ ëª¨ë¸ì— í™œìš©í• ìˆ˜ë„ ?


# In[26]:


q_data = df['question']
a_data = df['answer']


# In[27]:


q_data


# In[28]:


# í˜•íƒœì†Œ ë¶„ì„ ë¶ˆëŸ¬ì˜¤ê¸°
from mecab import MeCab
m = MeCab()

# ê° ì§ˆë¬¸ë§ˆë‹¤ í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ì§ˆë¬¸ë‹¹ ëª‡ ê°œì˜ ë‹¨ì–´ í† í°ì´ ë“¤ì–´ê°”ëŠ”ì§€ countë¦¬ìŠ¤íŠ¸ì— ë‹´ìŒ
count = []
for q in q_data:
        count_num = len(m.morphs(q))
        count.append(count_num)
len(count)


# In[29]:


question = df['question']
answer = df['answer']
intention = df['intention']
df['answer']


# In[30]:


# ê° ì§ˆë¬¸ë§ˆë‹¤ í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•´ ë‹µë³€ë‹¹ ëª‡ ê°œì˜ ë‹¨ì–´ í† í°ì´ ë“¤ì–´ê°”ëŠ”ì§€ count1 ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŒ

count1 = []
for a in a_data:
    count_num = len(m.morphs(a))
    count1.append(count_num)
len(count1)


# In[31]:


# ì§ˆë¬¸ì— ì‚¬ìš©ëœ ë‹¨ì–´ ê°œìˆ˜

import matplotlib.pyplot as plt

plt.hist(count)
point_6 = np.percentile(count, q=[0, 50, 75, 90, 95, 99]) # ìƒìœ„ 0%, 50%, 75%, 90%, 95%, 99% êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¶„í¬ ê·¸ë¦¬ê¸°
print(point_6)


# In[32]:


# ë‹µë³€ì— ì‚¬ìš©ëœ ë‹¨ì–´ ê°œìˆ˜ ()

plt.hist(count1)
point1_6 = np.percentile(count1, q=[0, 50, 75, 90, 95, 99]) # ìƒìœ„ 0%, 50%, 75%, 90%, 95%, 99% êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¶„í¬ ê·¸ë¦¬ê¸°
print(point1_6)


# In[33]:


import re

# í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±, ?!.,ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¸ì ì œê±°
korean_pattern = r'[^ ?,.!A-Za-z0-9ê°€-í£+]'

# íŒ¨í„´ ì»´íŒŒì¼
normalizer = re.compile(korean_pattern)
normalizer


# In[34]:


# ë¶ˆìš©ì–´ ì²˜ë¦¬ (ê·¸ëŸ°ë° ì´ë¯¸ ë¶ˆìš©ì–´ ì²˜ë¦¬ëœ ë°ì´í„°ë¼ ë³€í™”ê°€ ê±°ì˜ ì—†ìŒ)
print(f'ìˆ˜ì • ì „: {question[20]}')
print(f'ìˆ˜ì • í›„: {normalizer.sub("", question[20])}')
print(f'ìˆ˜ì • ì „: {answer[20]}')
print(f'ìˆ˜ì • í›„: {normalizer.sub("", answer[20])}')


# In[35]:


def normalize(sentence):
    return normalizer.sub("", sentence)

normalize(question[20])


# In[36]:


# í˜•íƒœì†Œ ë¶„ì„ ëŒë ¤ë³´ê¸°
mecab = MeCab()
mecab.morphs(normalize(question[20]))


# In[37]:


# í•œê¸€ ì „ì²˜ë¦¬ë¥¼ í•¨ìˆ˜í™”
def clean_text(sentence, mecab):
    sentence = normalize(sentence)
    sentence = mecab.morphs(sentence)
    sentence = ' '.join(sentence)
    sentence = sentence.lower()
    return sentence


# In[38]:


# í•œê¸€
clean_text(question[20], mecab)
clean_text(answer[20], mecab)


# In[39]:


# ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í˜•íƒœì†Œ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ê°ê° ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ë‹´ìŒ
questions = [clean_text(sent, mecab) for sent in question.values[:len(question)]]
answers = [clean_text(sent, mecab) for sent in answer.values[:len(question)]]


# In[40]:


questions[:5]


# In[41]:


answers[:5]


# ## seq2seq ëª¨ë¸
#  1. ì§ˆë¬¸ê³¼ ë‹µë³€ ë‚´ìš©ì„ ëª¨ë‘ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ í° ë‹¨ì–´ì‚¬ì „ì„ ë§Œë“¬

# In[42]:


import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torch.utils.data.dataset import Dataset

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device


# In[43]:


PAD_TOKEN = 0 # ë¹ˆê³µê°„ ì±„ì›Œì£¼ëŠ” í† í°
SOS_TOKEN = 1 # ë¬¸ì¥ì˜ ì‹œì‘ì ì„ í‘œì‹œí•˜ëŠ” í† í°
EOS_TOKEN = 2 # ë¬¸ì¥ì˜ ëì„ í‘œì‹œí•˜ëŠ” í† í°

# ë‹¨ì–´ì‚¬ì „ í´ë˜ìŠ¤
class WordVocab():
    def __init__(self):
        self.word2index = {
            '<PAD>': PAD_TOKEN,
            '<SOS>': SOS_TOKEN, 
            '<EOS>': EOS_TOKEN,
        }
        self.word2count = {}
        self.index2word = {
            PAD_TOKEN: '<PAD>', 
            SOS_TOKEN: '<SOS>', 
            EOS_TOKEN: '<EOS>'
        }
        
        self.n_words = 3  # PAD, SOS, EOS í¬í•¨

    def add_sentence(self, sentence):
        
        # for word in sentence.split(' '):
        for word in mecab.morphs(sentence): # ë¬¸ì¥ì„ í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜ì— ëŒë¦¬ë©´ ê° í˜•íƒœì†Œê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ê°€ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ë°”ë¡œ ë°˜ë³µë¬¸ìœ¼ë¡œ í•˜ë‚˜ì”© ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€í•´ì¤€ë‹¤.
            self.add_word(word)

    def add_word(self, word): # word2index : ë‹¨ì–´ë¥¼ ë²ˆí˜¸ë¡œ ë°”ê¿”ì£¼ëŠ” ë”•ì…”ë„ˆë¦¬, word2count : í•´ë‹¹ ë‹¨ì–´ê°€ ëª‡ë²ˆ ì“°ì˜€ëŠ”ì§€ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë”•ì…”ë„ˆë¦¬, index2word : ë²ˆí˜¸ë¥¼ ë‹¨ì–´ë¡œ ë°”ê¿”ì£¼ëŠ” ë”•ì…”ë„ˆë¦¬
        if word not in self.word2index: # í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¨ì–´ì‚¬ì „ì— ì—†ëŠ” ê²½ìš° ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì£¼ê³ , ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€í•œë‹¤.
            self.word2index[word] = self.n_words 
            self.word2count[word] = 1 
            self.index2word[self.n_words] = word
            self.n_words += 1 
        else:
            self.word2count[word] += 1 


# In[44]:


# í…ŒìŠ¤íŠ¸ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ lang1ì´ë€ ë‹¨ì–´ì‚¬ì „ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ì„œ ë„£ì–´ë³´ê¸°
lang1 = WordVocab()
for q in question:
    lang1.add_sentence(q)
for a in answer:
    lang1.add_sentence(a)
    


# In[45]:


lang1.word2index


# In[46]:


# ë¬¸ì¥ í•˜ë‚˜ ê°€ì ¸ì™€ì„œ ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€í•´ì„œ í™•ì¸ (í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ)
print(f'ì›ë¬¸: {questions[550]}')
lang = WordVocab()
lang.add_sentence(questions[550])
print('==='*10)
print('[ë‹¨ì–´ì‚¬ì „]')
print('***'*10)
print(lang.word2index)


# In[47]:


# ë¬¸ì¥ ìƒì„± í…ŒìŠ¤íŠ¸(ë‚˜ì¤‘ì— ë°ì´í„°ì…‹ ë§Œë“¤ë•Œ í•„ìš”í•œ ê³¼ì •)

max_length = 50 # ë¬¸ì¥ì„ ë‹´ì„ ê¸¸ì´ë¥¼ ì •í•˜ê¸°
sentence_length = 30 # ì…ë ¥í•  ë¬¸ì¥ê¸¸ì´

sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,)) # ëœë¤ìœ¼ë¡œ ë‹¨ì–´ ë½‘ì•„ì˜¤ê¸°(3ë²ˆ~100ê¹Œì§€)
sentence_tokens = sentence_tokens.tolist() 
print(f'Generated Sentence: {sentence_tokens}')

sentence_tokens = sentence_tokens[:(max_length-1)]

token_length = len(sentence_tokens)

# ë¬¸ì¥ì˜ ë§¨ ëë¶€ë¶„ì— <EOS> í† í° ì¶”ê°€
sentence_tokens.append(2)

for i in range(token_length, max_length-1):
    # ë‚˜ë¨¸ì§€ ë¹ˆ ê³³ì— <PAD> í† í° ì¶”ê°€
    sentence_tokens.append(0)

print(f'Output: {sentence_tokens}')
print(f'Total Length: {len(sentence_tokens)}')


# ## í•™ìŠµìš© ë°ì´í„° ë§Œë“¤ê¸°(ì—¬ê¸°ê°€ ì¤‘ìš”!)
#  1. csvíŒŒì¼ì—ì„œ ì§ˆë¬¸, ë‹µë³€ ë°ì´í„°ë¥¼ ì½ì–´ì™€ ë°ì´í„° í”„ë ˆì„ì„ ë§Œë“¬
#  2. ì§ˆë¬¸, ë‹µë³€ë‚´ìš©ì—ì„œ í˜•íƒœì†Œ ë¶„ì„(mecab ì‚¬ìš©)ì„ ì´ìš©í•´ ë‹¨ì–´ì‚¬ì „ì„ ë§Œë“¬
#  3. ì§ˆë¬¸ë‚´ìš©ì€ 30ê°œ í† í°, ë‹µë³€ ë‚´ìš©ì€ 300ê°œ í† í°ì„ ì‚¬ìš© (q_max_length, a_max_lengthë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ)
#  4. ì…ë ¥ ë¬¸ì¥ì„ ë‹¨ì–´ì‚¬ì „ì„ ì´ìš©í•´ ìˆ«ìë¡œ ë³€í™˜í•œ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¬ ex) "ì¹˜ë§¤ì— ì¢‹ì€ ìš´ë™ì€ ë­ê°€ ìˆë‚˜ìš”?" -> [ì¹˜ë§¤ì—, ì¢‹ì€, ìš´ë™, ì€, ë­ê°€, ìˆë‚˜ìš”?] -> [2, 4, 5, 6, 7, 8, 0.....,0](ê¸¸ì´ê°€ 30ì¸ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
#  5. ë‹µë³€ ë¬¸ì¥ë„ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê¸¸ì´ê°€ 300ì¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
#  6. ì˜ë„ëŠ” ê° ë‹¨ì–´ë§ˆë‹¤ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ í•˜ë‚˜ì˜ ìˆ«ìë¡œ ë³€í™˜
#  7. ì¶œë ¥ì€ {"answer" : ë‹µë³€ë‚´ìš©ë¬¸ìì—´, "intention" : ì˜ë„ì— í•´ë‹¹í•˜ëŠ” ìˆ«ì} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì—ˆìŒ
#  8. ê·¸ë¦¬ê³  ê° inputê°’ê³¼ ì¶œë ¥ê°’ì€ í•™ìŠµì‹œí‚¤ë ¤ë©´ tensorë¡œ ë³€í™˜ì‹œì¼œì¤˜ì•¼í•˜ê¸° ë•Œë¬¸ì—, torch.tensor()í•¨ìˆ˜ë¡œ ê°ì‹¸ì„œ í…ì„œí˜•íƒœë¡œ ë§Œë“¤ì–´ì¤Œ

# In[48]:


class TextDataset(Dataset):
    def __init__(self, csv_path, min_length=3, max_length1=50, q_max_length=30, a_max_length=300):
        super(TextDataset, self).__init__()
        # data_dir = 'data'
        
        # TOKEN ì •ì˜
        self.PAD_TOKEN = 0 # Padding í† í°
        self.SOS_TOKEN = 1 # SOS í† í°
        self.EOS_TOKEN = 2 # EOS í† í°
        
        self.tagger = MeCab()   # í˜•íƒœì†Œ ë¶„ì„ê¸°
        self.max_length1 = max_length # í•œ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ ì§€ì •
        self.q_max_length = q_max_length # ì§ˆë¬¸ ê¸¸ì´ ìµœëŒ€ ì§€ì •
        self.a_max_length = a_max_length # ë‹µë³€ ê¸¸ì´ ìµœëŒ€ ì§€ì •
        
        # CSV ë°ì´í„° ë¡œë“œ
        # df = pd.read_csv(os.path.join(data_dir, csv_path))
        df=pd.read_csv('dementia_fix.csv') # ì§ˆë¬¸, ë‹µë³€, ì˜ë„ê°€ ì €ì¥ëœ csvíŒŒì¼
        # í•œê¸€ ì •ê·œí™”
        korean_pattern = r'[^ ?,.!A-Za-z0-9ê°€-í£+]'
        self.normalizer = re.compile(korean_pattern)
        
        # src: ì§ˆì˜, itn: ì˜ë„ tgt: ë‹µë³€
        src_clean = []
        itn_clean = [] 
        tgt_clean = []
        
        # ë‹¨ì–´ ì‚¬ì „ ìƒì„±
        wordvocab = WordVocab()
        itn_label = {"ê²€ì§„" : 0, "ì‹ì´, ìƒí™œ" : 1, "ì•½ë¬¼" : 2, "ì˜ˆë°©" : 3, "ìš´ë™" : 4, "ì›ì¸" : 5, "ì •ì˜" : 6, "ì¦ìƒ" : 7, "ì§„ë‹¨" : 8, "ì¹˜ë£Œ" : 9}
        for _, row in df.iterrows():
            src = row['question']
            itn = row['intention']
            tgt = row['answer']
            
            # í•œê¸€ ì „ì²˜ë¦¬
            src = self.clean_text(src)
            tgt = self.clean_text(tgt)
            
            if len(src.split()) > min_length and len(tgt.split()) > min_length:
                # ìµœì†Œ ê¸¸ì´ë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì˜ ë‹¨ì–´ë§Œ ì¶”ê°€
                wordvocab.add_sentence(src)
                wordvocab.add_sentence(tgt)
                src_clean.append(src)          
                tgt_clean.append(tgt)
            itn_clean.append(itn_label[itn])
        
        self.srcs = src_clean
        self.itns = itn_clean
        self.tgts = tgt_clean
        self.wordvocab = wordvocab
    
    def normalize(self, sentence):
        # ì •ê·œí‘œí˜„ì‹ì— ë”°ë¥¸ í•œê¸€ ì •ê·œí™”
        return self.normalizer.sub("", sentence)

    def clean_text(self, sentence):
        # í•œê¸€ ì •ê·œí™”
        sentence = self.normalize(sentence)
        # í˜•íƒœì†Œ ì²˜ë¦¬
        sentence = self.tagger.morphs(sentence)
        sentence = ' '.join(sentence)
        sentence = sentence.lower()
        return sentence
    
    def texts_to_sequences(self, sentence):
        # ë¬¸ì¥ -> ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
        return [self.wordvocab.word2index[w] for w in mecab.morphs(sentence)]
    
        # return [self.wordvocab.word2index[w] for w in sentence.split()]

    def pad_sequence(self, sentence_tokens, max_length):
        # ë¬¸ì¥ì˜ ë§¨ ë í† í°ì€ ì œê±°
        sentence_tokens = sentence_tokens[:(max_length-1)]
        token_length = len(sentence_tokens)

        # ë¬¸ì¥ì˜ ë§¨ ëë¶€ë¶„ì— <EOS> í† í° ì¶”ê°€
        sentence_tokens.append(self.EOS_TOKEN)

        for i in range(token_length, (max_length-1)):
            # ë‚˜ë¨¸ì§€ ë¹ˆ ê³³ì— <PAD> í† í° ì¶”ê°€
            sentence_tokens.append(self.PAD_TOKEN)
        return sentence_tokens
    
    def __getitem__(self, idx):
        # ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°
        #------------------------------------
        # 1 | ì§ˆë¬¸ë‚´ìš© | ì˜ë„ | ë‹µë³€ë‚´ìš© 
        # 2 | ì§ˆë¬¸ë‚´ìš© | ì˜ë„ | ë‹µë³€ë‚´ìš© 
        # ...
        # ë§ˆì§€ë§‰ idx | ì§ˆë¬¸ë‚´ìš© | ì˜ë„ | ë‹µë³€ë‚´ìš© 
        #-------------------------------------
        # ì—¬ê¸°ì„œ í•œì¤„ì”© ë½‘ì•„ì„œ í•™ìŠµìš© ë°ì´í„°ì…‹ì„ êµ¬ì„±
        # inputs = self.srcs[idx]
        
        # ì…ë ¥í˜•íƒœ ë§Œë“¤ê¸°
        inputs_sequences = self.texts_to_sequences(self.srcs[idx])
        inputs_padded = self.pad_sequence(inputs_sequences, self.q_max_length)
        
        # outputs = self.tgts[idx]
        # ì¶œë ¥ í˜•íƒœ ë§Œë“¤ê¸°
        outputs = {}
        outputs_sequences = self.texts_to_sequences(self.tgts[idx])
        outputs_padded = self.pad_sequence(outputs_sequences, self.a_max_length)
        outputs['answer'] = torch.tensor(outputs_padded)
        outputs['intention'] = torch.tensor(self.itns[idx])
        
        return torch.tensor(inputs_padded), outputs
        # return torch.tensor(inputs_padded), torch.tensor(outputs_padded)
    
    def __len__(self):
        return len(self.srcs)


# In[49]:


# í•œ ë¬¸ì¥ì˜ ìµœëŒ€ ë‹¨ì–´ê¸¸ì´ë¥¼ 300ë¡œ ì„¤ì •
MAX_LENGTH = 300 # ì•ˆì¨ë„ ë˜ëŠ” íŒŒë¼ë¯¸í„°
Q_MAX_LENGTH = 30 # ì§ˆë¬¸ ë¬¸ì¥ ìµœëŒ€ 30ê°œ í† í° ì‚¬ìš©
A_MAX_LENGTH = 350 # ë‹µë³€ ë¬¸ì¥ ìµœëŒ€ 350ê°œ í† í° ì‚¬ìš©

dataset = TextDataset('dementia_fix.csv', min_length=3, max_length1=MAX_LENGTH, q_max_length=Q_MAX_LENGTH, a_max_length=A_MAX_LENGTH)


# In[50]:


# ë‹¨ì–´ì‚¬ì „ ë“±ë¡ëœ ë‹¨ì–´ ê°œìˆ˜
dataset.wordvocab.n_words


# In[51]:


# ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼ í™•ì¸
dataset[0]


# In[52]:


# 10ë²ˆì§¸ ë°ì´í„° ì„ì˜ ì¶”ì¶œ
x, y = dataset[10]


# In[53]:


x


# In[54]:


print(f'x shape: {x.shape}')
print(x)

print(f'y shape: {y["answer"].shape}')
print(y)


# In[55]:


# 80%ì˜ ë°ì´í„°ë¥¼ trainì— í• ë‹¹í•©ë‹ˆë‹¤.
train_size = int(len(dataset) * 0.8)
train_size


# In[56]:


# ë‚˜ë¨¸ì§€ 20% ë°ì´í„°ë¥¼ testì— í• ë‹¹í•©ë‹ˆë‹¤.
test_size = len(dataset) - train_size
test_size


# In[57]:


from torch.utils.data import random_split

# ëœë¤ ìŠ¤í”Œë¦¿ìœ¼ë¡œ ë¶„í• ì„ ì™„ë£Œí•©ë‹ˆë‹¤.
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])


# In[58]:


from torch.utils.data import DataLoader, SubsetRandomSampler

# ë°°ì¹˜ì‚¬ì´ì¦ˆ : 16 (16ê°œì”© ë¬¶ìŒ)
train_loader = DataLoader(train_dataset, 
                          batch_size=16, 
                          shuffle=True)

test_loader = DataLoader(test_dataset, 
                         batch_size=16, 
                         shuffle=True)


# In[59]:


train_dataset[0]


# In[60]:


# 1ê°œì˜ ë°°ì¹˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
x, y = next(iter(train_loader))


# In[61]:


x, y


# In[62]:


# shape: (batch_size, sequence_length)
x.shape, y["answer"].shape


# ## ëª¨ë¸ ë§Œë“¤ê¸°
#  - ê¸°ë³¸ì ìœ¼ë¡œ seq-to-seq ëŠ” encoder - decoder êµ¬ì¡°ë¡œ ë˜ì–´ìˆìŒ
#  - í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì„ë² ë”©ì„ í†µí•´ ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤Œ
#  - ë‹¨ì–´ì‚¬ì „ì˜ ìˆëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ì„ë² ë”©ì„ í•´ì¤Œ
#  - ë°°ì¹˜ì‚¬ì´ì¦ˆê°€ 16ê°œì´ë¯€ë¡œ, ì§ˆë¬¸ì…ë ¥ ì‚¬ì´ì¦ˆëŠ”(16x30), ë‹µë³€ ì…ë ¥ì‚¬ì´ì¦ˆëŠ”(16x300)
#  - ì—¬ê¸°ì— ì„ë² ë”© ì°¨ì›ì´ 64ì´ë¯€ë¡œ(ë‹¨ì–´ í•˜ë‚˜ë¥¼ 64ê°œì˜ ë¬´ì–¸ê°€ë¡œ í‘œí˜„) ê°ê° ì¸ì½”ë”ë¥¼ í†µê³¼í•˜ë©´ (16x30x64),(16x300x64)ì˜ ì‚¬ì´ì¦ˆê°€ ëœë‹¤.
#  - gruì˜ íˆë“  ë ˆì´ì–´ ì‚¬ì´ì¦ˆê°€ 32ì´ë¯€ë¡œ, ì„ë² ë”©ì„ í†µê³¼í•œ ë°ì´í„°ê°€ 64ì—ì„œ 32ë¡œ ì¤„ì–´ë“¬ (16x30x32), (16x300x32)

# In[63]:


class Encoder(nn.Module):
    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):
        super(Encoder, self).__init__()
        
        # ë‹¨ì–´ ì‚¬ì „ì˜ ê°œìˆ˜ ì§€ì •
        self.num_vocabs = num_vocabs
        # ì„ë² ë”© ë ˆì´ì–´ ì •ì˜ (number of vocabs, embedding dimension)
        self.embedding = nn.Embedding(num_vocabs, embedding_dim)
        # GRU (embedding dimension)
        self.gru = nn.GRU(embedding_dim, 
                          hidden_size, 
                          num_layers=num_layers, 
                          bidirectional=False)
        
    def forward(self, x):
        x = self.embedding(x).permute(1, 0, 2)
        output, hidden = self.gru(x)
        return output, hidden


# In[64]:


# Embedding Layerì˜ ì…/ì¶œë ¥ shapeì— ëŒ€í•œ ì´í•´

embedding_dim = 64 # ì„ë² ë”© ì°¨ì›
embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)

# xì˜ shapeì„ ë³€ê²½í•©ë‹ˆë‹¤.
# (batch_size, sequence_length) => (sequence_length, batch_size)
embedded = embedding(x)

print(x.shape)
print(embedded.shape)
# input:  (sequence_length, batch_size)
# output: (sequence_length, batch_size, embedding_dim)


# In[65]:


embedded = embedded.permute(1, 0, 2)
print(embedded.shape)
# (sequence_length, batch_size, embedding_dim)


# In[66]:


hidden_size = 32   

gru = nn.GRU(embedding_dim,      # embedding ì°¨ì›
             hidden_size, 
             num_layers=1, 
             bidirectional=False)

# input       : (sequence_length, batch_size, embedding_dim)
# h0          : (Bidirectional(1) x number of layers(1), batch_size, hidden_size)
o, h = gru(embedded, None)

print(o.shape)
print(h.shape)
# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))
# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size)


# In[67]:


NUM_VOCABS = dataset.wordvocab.n_words
print(f'number of vocabs: {NUM_VOCABS}')


# In[68]:


# Encoder ì •ì˜
encoder = Encoder(NUM_VOCABS, 
                  hidden_size=32, 
                  embedding_dim=64, 
                  num_layers=1)


# In[69]:


# Encoderì— x í†µê³¼ í›„ output, hidden_size ì˜ shape í™•ì¸
# input(x)    : (batch_size, sequence_length)
o, h = encoder(x)

print(o.shape)
print(h.shape)
# output      : (sequence_length, batch_size, hidden_size x bidirectional(1))
# hidden_state: (bidirectional(1) x number of layers(1), batch_size, hidden_size


# In[70]:


class Decoder(nn.Module):
    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1, dropout=0.2):
        super(Decoder, self).__init__()
        # ë‹¨ì–´ì‚¬ì „ ê°œìˆ˜
        self.num_vocabs = num_vocabs
        self.embedding = nn.Embedding(num_vocabs, embedding_dim)
        self.dropout = nn.Dropout(dropout)
        self.gru = nn.GRU(embedding_dim, 
                          hidden_size, 
                          num_layers=num_layers, 
                          bidirectional=False)
        
        # ìµœì¢… ì¶œë ¥ì€ ë‹¨ì–´ì‚¬ì „ì˜ ê°œìˆ˜
        self.fc = nn.Linear(hidden_size, num_vocabs)
        
    def forward(self, x, hidden_state):
        x = x.unsqueeze(0) # (1, batch_size) ë¡œ ë³€í™˜
        embedded = F.relu(self.embedding(x))
        embedded = self.dropout(embedded)
        output, hidden = self.gru(embedded, hidden_state)
        output = self.fc(output.squeeze(0)) # (sequence_length, batch_size, hidden_size(32) x bidirectional(1))
        return output, hidden


# In[71]:


#Embedding Layerì˜ ì…/ì¶œë ¥ shape
x = torch.abs(torch.randn(size=(1, 16)).long())
print(x)
x.shape
# batch_size = 16 ì´ë¼ ê°€ì •í–ˆì„ ë•Œ,
# (1, batch_size)
# ì—¬ê¸°ì„œ batch_size => (1, batch_size) ë¡œ shape ë³€í™˜ì„ ì„ í–‰


# In[72]:


embedding_dim = 64 # ì„ë² ë”© ì°¨ì›
embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)

embedded = embedding(x)
embedded.shape
# embedding ì¶œë ¥
# (1, batch_size, embedding_dim)


# In[73]:


#GRU Layerì˜ ì…/ì¶œë ¥ shapeì— ëŒ€í•œ ì´í•´
hidden_size = 32

gru = nn.GRU(embedding_dim, 
            hidden_size, 
            num_layers=1, 
            bidirectional=False, 
            batch_first=False, # batch_first=Falseë¡œ ì§€ì •
           )

o, h = gru(embedded)

print(o.shape)
# output shape: (sequence_length, batch_size, hidden_size(32) x bidirectional(1))
print(h.shape)
# hidden_state shape: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))


# In[74]:


# ìµœì¢… ì¶œë ¥ì¸µ(FC) shapeì— ëŒ€í•œ ì´í•´
fc = nn.Linear(32, NUM_VOCABS) # ì¶œë ¥ì€ ë‹¨ì–´ì‚¬ì „ì˜ ê°œìˆ˜ë¡œ ê°€ì •

output = fc(o[0])

print(o[0].shape)
print(output.shape)
# input : (batch_size, output from GRU)
# output: (batch_size, output dimension)


# In[75]:


#ì¸ì½”ë” -> ë””ì½”ë” ì…ì¶œë ¥ shape
decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, 
                  hidden_size=32, 
                  embedding_dim=64, 
                  num_layers=1)


# In[76]:


x, y = next(iter(train_loader))

o, h = encoder(x)

print(o.shape, h.shape)
# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))
# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))


# In[77]:


# ***************
x = torch.abs(torch.full(size=(16,), fill_value=SOS_TOKEN, dtype=torch.long))
print(x)
x.shape

# batch_size = 16 ì´ë¼ ê°€ì •(16ê°œì˜ SOS í† í°)


# In[78]:


embedding_dim = 64 # ì„ë² ë”© ì°¨ì›
embedding = nn.Embedding(dataset.wordvocab.n_words, embedding_dim)

embedded = embedding(x)
embedded.shape
# embedding ì¶œë ¥
# (1, batch_size, embedding_dim)


# In[79]:


x


# In[80]:


decoder_output, decoder_hidden = decoder(x, h)
decoder_output.shape, decoder_hidden.shape
# (batch_size, num_vocabs), (1, batch_size, hidden_size)


# In[81]:


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):
        # inputs : (batch_size, sequence_length)
        # outputs: (batch_size, sequence_length)
        
        batch_size, output_length = outputs.shape
        output_num_vocabs = self.decoder.num_vocabs
        
        # ë¦¬í„´í•  ì˜ˆì¸¡ëœ outputsë¥¼ ì €ì¥í•  ì„ì‹œ ë³€ìˆ˜
        # (sequence_length, batch_size, num_vocabs)
        predicted_outputs = torch.zeros(output_length, batch_size, output_num_vocabs).to(self.device)
        
        # ì¸ì½”ë”ì— ì…ë ¥ ë°ì´í„° ì£¼ì…, encoder_outputì€ ë²„ë¦¬ê³  hidden_state ë§Œ ì‚´ë¦½ë‹ˆë‹¤. 
        # ì—¬ê¸°ì„œ hidden_stateê°€ ë””ì½”ë”ì— ì£¼ì…í•  context vector ì…ë‹ˆë‹¤.
        # (Bidirectional(1) x number of layers(1), batch_size, hidden_size)
        _, decoder_hidden = self.encoder(inputs)
        
        # (batch_size) shapeì˜ SOS TOKENìœ¼ë¡œ ì±„ì›Œì§„ ë””ì½”ë” ì…ë ¥ ìƒì„±********************
        decoder_input = torch.full((batch_size,), SOS_TOKEN, dtype=torch.long, device=self.device)
        
        # ìˆœíšŒí•˜ë©´ì„œ ì¶œë ¥ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        # 0ë²ˆì§¸ëŠ” SOS TOKENì´ ìœ„ì¹˜í•˜ë¯€ë¡œ, 1ë²ˆì§¸ ì¸ë±ìŠ¤ë¶€í„° ìˆœíšŒí•©ë‹ˆë‹¤.
        for t in range(0, output_length):
            # decoder_input : ë””ì½”ë” ì…ë ¥ (batch_size) í˜•íƒœì˜ SOS TOKENë¡œ ì±„ì›Œì§„ ì…ë ¥
            # decoder_output: (batch_size, num_vocabs)
            # decoder_hidden: (Bidirectional(1) x number of layers(1), batch_size, hidden_size), context vectorì™€ ë™ì¼ shape
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)

            # të²ˆì§¸ ë‹¨ì–´ì— ë””ì½”ë”ì˜ output ì €ì¥
            predicted_outputs[t] = decoder_output
            
            # teacher forcing ì ìš© ì—¬ë¶€ í™•ë¥ ë¡œ ê²°ì •
            # teacher forcing ì´ë€: ì •ë‹µì¹˜ë¥¼ ë‹¤ìŒ RNN Cellì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ê²½ìš°. ìˆ˜ë ´ì†ë„ê°€ ë¹ ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
            teacher_force = random.random() < teacher_forcing_ratio
            
            # top1 ë‹¨ì–´ í† í° ì˜ˆì¸¡
            top1 = decoder_output.argmax(1) 
            
            # teacher forcing ì¸ ê²½ìš° ground truth ê°’ì„, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°, ì˜ˆì¸¡ ê°’ì„ ë‹¤ìŒ inputìœ¼ë¡œ ì§€ì •
            decoder_input = outputs[:, t] if teacher_force else top1
        
        return predicted_outputs.permute(1, 0, 2) # (batch_size, sequence_length, num_vocabs)ë¡œ ë³€ê²½


# In[82]:


#Seq2Seq ì…ì¶œë ¥ í™•ì¸
# Encoder ì •ì˜
encoder = Encoder(num_vocabs=dataset.wordvocab.n_words, 
                       hidden_size=32, 
                       embedding_dim=64, 
                       num_layers=1)
# Decoder ì •ì˜
decoder = Decoder(num_vocabs=dataset.wordvocab.n_words, 
                       hidden_size=32, 
                       embedding_dim=64, 
                       num_layers=1)
# Seq2Seq ì •ì˜
seq2seq = Seq2Seq(encoder, decoder, 'cpu')


# In[83]:


x, y = next(iter(train_loader))
# print(x.shape, y.shape)
# (batch_size, sequence_length), (batch_size, sequence_length)


# In[84]:


x


# In[85]:


y


# In[86]:


import random
output = seq2seq(x, y['answer'])
# print(output.shape)
# (batch_size, sequence_length, num_vocabs)


# In[87]:


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

NUM_VOCABS = dataset.wordvocab.n_words
HIDDEN_SIZE = 512
EMBEDDIMG_DIM = 256

print(f'num_vocabs: {NUM_VOCABS}\n======================')

# Encoder ì •ì˜
encoder = Encoder(num_vocabs=NUM_VOCABS, 
                  hidden_size=HIDDEN_SIZE, 
                  embedding_dim=EMBEDDIMG_DIM, 
                  num_layers=1)
# Decoder ì •ì˜
decoder = Decoder(num_vocabs=NUM_VOCABS, 
                  hidden_size=HIDDEN_SIZE, 
                  embedding_dim=EMBEDDIMG_DIM, 
                  num_layers=1)

# Seq2Seq ìƒì„±
# encoder, decoderë¥¼ device ëª¨ë‘ ì§€ì •
model = Seq2Seq(encoder.to(device), decoder.to(device), device)
print(model)


# In[88]:


encoder


# In[89]:


class EarlyStopping:
    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):
        """
        patience (int): loss or scoreê°€ ê°œì„ ëœ í›„ ê¸°ë‹¤ë¦¬ëŠ” ê¸°ê°„. default: 3
        delta  (float): ê°œì„ ì‹œ ì¸ì •ë˜ëŠ” ìµœì†Œ ë³€í™” ìˆ˜ì¹˜. default: 0.0
        mode     (str): ê°œì„ ì‹œ ìµœì†Œ/ìµœëŒ€ê°’ ê¸°ì¤€ ì„ ì •('min' or 'max'). default: 'min'.
        verbose (bool): ë©”ì‹œì§€ ì¶œë ¥. default: True
        """
        self.early_stop = False
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        
        self.best_score = np.inf if mode == 'min' else 0
        self.mode = mode
        self.delta = delta
        

    def __call__(self, score):

        if self.best_score is None:
            self.best_score = score
            self.counter = 0
        elif self.mode == 'min':
            if score < (self.best_score - self.delta):
                self.counter = 0
                self.best_score = score
                if self.verbose:
                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')
            else:
                self.counter += 1
                if self.verbose:
                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \
                          f'Best: {self.best_score:.5f}' \
                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')
                
        elif self.mode == 'max':
            if score > (self.best_score + self.delta):
                self.counter = 0
                self.best_score = score
                if self.verbose:
                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')
            else:
                self.counter += 1
                if self.verbose:
                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \
                          f'Best: {self.best_score:.5f}' \
                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')
                
            
        if self.counter >= self.patience:
            if self.verbose:
                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')
            # Early Stop
            self.early_stop = True
        else:
            # Continue
            self.early_stop = False


# ## í•™ìŠµ ë° ëª¨ë¸ íŠœë‹
#  - í•™ìŠµì‹œí‚¤ë©´ì„œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •

# In[90]:


# í›ˆë ¨ì— ì ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

LR = 1e-3
optimizer = optim.Adam(model.parameters(), lr=LR)
loss_fn = nn.CrossEntropyLoss()

es = EarlyStopping(patience=5, 
                   delta=0.001, 
                   mode='min', 
                   verbose=True
                  )

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 
                                                 mode='min', 
                                                 factor=0.5, 
                                                 patience=2,
                                                 threshold_mode='abs',
                                                 min_lr=1e-8, 
                                                 verbose=True)


# In[91]:


# í›ˆë ¨ì— ì ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

LR = 5e-4
optimizer = optim.Adam(model.parameters(), lr=LR)
loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)

es = EarlyStopping(patience=7, 
                   delta=0.01, 
                   mode='min', 
                   verbose=True
                  )

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 
                                                 mode='min', 
                                                 factor=0.5, 
                                                 patience=3,
                                                 threshold_mode='abs',
                                                 min_lr=1e-8, 
                                                 verbose=True)


# In[92]:


# train í•¨ìˆ˜ ì •ì˜
def train(model, data_loader, optimizer, loss_fn, device):
    model.train()
    running_loss = 0
    
    for x, y in data_loader:
        x, y = x.to(device), y['answer'].to(device) # tensorë¡œë§Œ í•™ìŠµì´ ë˜ê¸°ë•Œë¬¸ì—, ë”•ì…”ë„ˆë¦¬ì•ˆì— ìˆëŠ” ë‹µë³€ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì™€ì•¼í•¨
        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)

        optimizer.zero_grad() # ì´ˆê¸°í™”
        
        # output: (batch_size, sequence_length, num_vocabs)
        output = model(x, y)
        output_dim = output.size(2)
        
        # 1ë²ˆ index ë¶€í„° ìŠ¬ë¼ì´ì‹±í•œ ì´ìœ ëŠ” 0ë²ˆ indexê°€ SOS TOKEN ì´ê¸° ë•Œë¬¸
        # (batch_size*sequence_length, num_vocabs) ë¡œ ë³€ê²½
        output = output.reshape(-1, output_dim)
        
        # (batch_size*sequence_length) ë¡œ ë³€ê²½
        y = y.view(-1)
        
        # Loss ê³„ì‚°
        loss = loss_fn(output, y)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item() * x.size(0)
        
    return running_loss / len(data_loader)


# In[93]:


# evaluation í•¨ìˆ˜ ì •ì˜
def evaluate(model, data_loader, loss_fn, device):
    model.eval()
    
    eval_loss = 0
    
    with torch.no_grad():
        for x, y in data_loader:
            x, y = x.to(device), y['answer'].to(device)
            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)
            output = model(x, y)
            output_dim = output.size(2)
            output = output.reshape(-1, output_dim)
            y = y.view(-1)
            
            # Loss ê³„ì‚°
            loss = loss_fn(output, y)
            
            eval_loss += loss.item() * x.size(0)
            
    return eval_loss / len(data_loader)


# In[94]:


# ëœë¤ ìƒ˜í”Œë§ í›„ ê²°ê³¼ ì¶”ë¡ 
def sequence_to_sentence(sequences, index2word):
    outputs = []
    for p in sequences:

        word = index2word[p]
        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:
            outputs.append(word)
        if word == EOS_TOKEN:
            break
    return ' '.join(outputs)


# In[95]:


# sequenceë¥¼ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¾¸ì–´ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜

def random_evaluation(model, dataset, index2word, device, n=10):
    
    n_samples = len(dataset)
    indices = list(range(n_samples))
    np.random.shuffle(indices)      # Shuffle
    sampled_indices = indices[:n]   # Sampling N indices
    
    # ìƒ˜í”Œë§í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DataLoader ìƒì„±
    sampler = SubsetRandomSampler(sampled_indices)
    sampled_dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)
    
    model.eval()
    with torch.no_grad():
        for x, y in sampled_dataloader:
            x, y = x.to(device), y['answer'].to(device)      
            # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  
            output = model(x, y, teacher_forcing_ratio=0)
            # output: (number of samples, sequence_length, num_vocabs)
            
            preds = output.detach().cpu().numpy()
            x = x.detach().cpu().numpy()
            y = y.detach().cpu().numpy()
            
            for i in range(n):
                print(f'ì§ˆë¬¸   : {sequence_to_sentence(x[i], index2word)}')
                print(f'ë‹µë³€   : {sequence_to_sentence(y[i], index2word)}')
                print(f'ì˜ˆì¸¡ë‹µë³€: {sequence_to_sentence(preds[i].argmax(1), index2word)}')
                print('==='*10)


# In[96]:


# ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ë‹µì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜

def predict(model, sentence, index2word, device, n=10):

    model.eval()
    with torch.no_grad():
        input_tokens = dataset.texts_to_sequences(dataset.clean_text(sentence))
        input_padded = dataset.pad_sequence(input_tokens, dataset.q_max_length)
    
        # ì…ë ¥ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
        input_tensor = torch.tensor(input_padded).unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•˜ê³  í…ì„œë¡œ ë³€í™˜
        output_tensor =  torch.tensor([0 for i in range(350)]).unsqueeze(0).to(device)
        # x, y['answer'], y['intention'] = x.to(device), y['answer'].to(device), y['intention'].to(device)  
        output = model(input_tensor, output_tensor, teacher_forcing_ratio=0)
        # output: (number of samples, sequence_length, num_vocabs)
        
        # preds = output.detach().cpu().numpy()
        # x = x.detach().cpu().numpy()
        # y = y.detach().cpu().numpy()
        
        output_tokens = output.detach().squeeze(0).cpu().numpy()  # ë°°ì¹˜ ì°¨ì›ì„ ì œê±°í•˜ê³  ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
        predicted_tokens = np.argmax(output_tokens, axis=1)  # ê° ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ í™•ë¥  í† í°ì„ ì„ íƒ
        
        response_sentence = sequence_to_sentence(predicted_tokens, index2word)  # í† í° ì‹œí€€ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        
        return response_sentence


# In[97]:


# #  í›ˆë ¨ ì‹œì‘
# NUM_EPOCHS = 5
# STATEDICT_PATH = 'seq2seq-chatbot-kor.pt'

# best_loss = np.inf

# for epoch in range(NUM_EPOCHS):
#     loss = train(model, train_loader, optimizer, loss_fn, device)
    
#     val_loss = evaluate(model, test_loader, loss_fn, device)
     
#     if val_loss < best_loss:
#         best_loss = val_loss
#         torch.save(model.state_dict(), STATEDICT_PATH)
    
#     if epoch % 5 == 0:
#         print(f'epoch: {epoch+1}, loss: {loss:.4f}, val_loss: {val_loss:.4f}')
    
#     # Early Stop
#     es(loss)
#     if es.early_stop:
#         break
    
#     # Scheduler
#     scheduler.step(val_loss)
                   
# model.load_state_dict(torch.load(STATEDICT_PATH))
# torch.save(model.state_dict(), f'seq2seq-chatbot-kor-{best_loss:.4f}.pt')


# In[98]:


import pickle
with open('dict.pkl', 'rb') as f:
    dict1=pickle.load(f)
    dict2 = {v : k for k, v in dict1.items()}
dataset.wordvocab.word2index = dict1
dataset.wordvocab.index2word = dict2


# In[99]:


STATEDICT_PATH = 'seq2seq-chatbot-kor22.pt'
model.load_state_dict(torch.load(STATEDICT_PATH, map_location=device))
random_evaluation(model, test_dataset, dataset.wordvocab.index2word, device)


# In[100]:


import gradio as gr
import random
import time
def response(message, history, additional_input_info):
    # additional_input_infoì˜ í…ìŠ¤íŠ¸ë¥¼ ì±—ë´‡ì˜ ëŒ€ë‹µ ë’¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    response = predict(model, message, dataset.wordvocab.index2word, device)
    return response
gr.ChatInterface(
        fn=response,
        textbox=gr.Textbox(placeholder="ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”", container=False, scale=7),
        title="ì¹˜ë§¤ë°•ì‚¬ğŸŒì•ˆê¹œë¹¡ì´ì—ìš”~",
        description="ë‡Œì‹ ê²½ì§ˆí™˜(ì¹˜ë§¤,ì•Œì½”ì˜¬ì„±ì¹˜ë§¤,ì•Œì¸ í•˜ì´ë¨¸ë³‘,ìš°ì„ì¦)ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.",
        theme="soft",
        examples=[["ì¹˜ë§¤ê°€ ì˜ì‹¬ë ë•ŒëŠ” ì–´ë–¤ ì „ë¬¸ì˜ë¥¼ ë°©ë¬¸í•´ì•¼ í•˜ë‚˜ìš”?"], ["ì•Œì½”ì˜¬ì„± ì¹˜ë§¤ë¥¼ ì˜ˆë°©í•˜ëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì€ìš”?"], ["ìš°ìš¸ì¦ ì•½ë¬¼ì˜ ë¶€ì‘ìš©ì€ìš”?"]],
        retry_btn="ë‹¤ì‹œë³´ë‚´ê¸° âœˆ",
        undo_btn="ì´ì „ì±— ì‚­ì œ âœ‚",
        clear_btn="ì „ì±— ì‚­ì œ ğŸ’¥",
        additional_inputs=[
            gr.Textbox("!!!", label="ëë§ì‡ê¸°")
        ]
).launch()


# In[ ]:





# In[ ]:




